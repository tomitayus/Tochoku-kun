# @title å½“ç›´ãã‚“ v3.4 (TARGET_CAPå³æ ¼åŒ–+å¤šè»¸ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°+å…¬å¹³æ€§å¼·åˆ¶)
# ä¿®æ­£å†…å®¹:
# v3.4 (2026-01-29):
# - TARGET_CAPé•åã®å³æ ¼åŒ–
#   - ãƒ‘ã‚¿ãƒ¼ãƒ³é¸æŠæ™‚ã« cap_violations > 0 ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å¤–
#   - gap_violations > 0ã€unassigned_slots > 0 ã‚‚é™¤å¤–
#   - ãƒãƒ¼ãƒ‰åˆ¶ç´„ã‚’æº€ãŸã™ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿ã‚’å€™è£œã¨ã—ã¦é¸æŠ
# - å¤šè»¸ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°ã‚·ã‚¹ãƒ†ãƒ ã‚’å®Ÿè£…
#   - å…¬å¹³æ€§é‡è¦–: TARGET_CAPã€åŒ»å¸«é–“ã®å‰²å½“å›æ•°ã®å…¬å¹³æ€§ã‚’æœ€å„ªå…ˆ
#   - é€£ç¶šå½“ç›´å›é¿é‡è¦–: gapé•åã€å¤–ç—…é™¢é‡è¤‡ã‚’æœ€å„ªå…ˆ
#   - ãƒãƒ©ãƒ³ã‚¹é‡è¦–: å¤§å­¦/å¤–ç—…é™¢ãƒãƒ©ãƒ³ã‚¹ã€å¹³æ—¥/ä¼‘æ—¥ãƒãƒ©ãƒ³ã‚¹ã‚’æœ€å„ªå…ˆ
#   - å„è»¸ã‹ã‚‰æœ€è‰¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’1ã¤ãšã¤é¸æŠã—ã€åˆè¨ˆ3ãƒ‘ã‚¿ãƒ¼ãƒ³å‡ºåŠ›
# - å…¬å¹³æ€§ã®å¼·åˆ¶ä¿®æ­£æ©Ÿèƒ½ã‚’è¿½åŠ 
#   - fix_fairness_imbalance: æœ€å¤§å‰²å½“å›æ•°ã¨æœ€å°å‰²å½“å›æ•°ã®å·®ã‚’ç¸®ã‚ã‚‹
#   - 4å›ã®åŒ»å¸«ã‹ã‚‰1å›ã®åŒ»å¸«ã«ã‚·ãƒ•ãƒˆã‚’ç§»å‹•ã—ã€å…¬å¹³æ€§ã‚’é”æˆ
#   - å·®ãŒ1ä»¥ä¸‹ã«ãªã‚‹ã¾ã§ä¿®æ­£ï¼ˆä¾‹: 1å›ã¨4å› â†’ 2å›ã¨3å›ï¼‰
#   - æœ€é©åŒ–ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã«çµ±åˆï¼ˆä¿®æ­£é †åºã®æœ€å¾Œã«å®Ÿè¡Œï¼‰
# v3.3 (2026-01-28):
# - å¤§å­¦ç—…é™¢3å›ä»¥ä¸Šã‚’ç¦æ­¢ï¼ˆä¸æº€ãŒé«˜ã„ï¼‰
#   - bg_over_2_violations: å¤§å­¦3å›ä»¥ä¸Šã®é•åã‚’æ¤œå‡ºï¼ˆãƒšãƒŠãƒ«ãƒ†ã‚£150ï¼‰
#   - fix_university_over_2_violations: æœ€é©åŒ–å¾Œã«å¤§å­¦3å›ä»¥ä¸Šã‚’ä¿®æ­£
#   - å¤§å­¦ã®å‰²å½“ã‚’å¤–ç—…é™¢ã«ç§»å‹•ã€ã¾ãŸã¯å‰Šé™¤ã—ã¦2å›ä»¥ä¸‹ã«åˆ¶é™
# - å¤§å­¦ç—…é™¢ã®å¹³æ—¥åã‚Šåˆ¶ç´„ã‚’è¿½åŠ ï¼ˆå¹³æ—¥2å›ä»¥ä¸Šã¯ä¸æº€ï¼‰
#   - bg_weekday_over_violations: å¤§å­¦ã®å¹³æ—¥2å›ä»¥ä¸Šã®é•åã‚’æ¤œå‡ºï¼ˆãƒšãƒŠãƒ«ãƒ†ã‚£80ï¼‰
#   - fix_university_weekday_balance_violations: æœ€é©åŒ–å¾Œã«å¹³æ—¥åã‚Šã‚’ä¿®æ­£
#   - å¤§å­¦å¹³æ—¥ã®å‰²å½“ã‚’å¤–ç—…é™¢ã«ç§»å‹•ã€ã¾ãŸã¯å‰Šé™¤
# - å…¨ä½“ã®å…¬å¹³æ€§ã‚’å¼·åŒ–ï¼ˆ2å›ã®åŒ»å¸«ãŒã„ã‚‹ãªã‚‰4å›ã®åŒ»å¸«ã‹ã‚‰æ¸¡ã™ï¼‰
#   - fairness_penaltyè¨ˆç®—ã‚’å¼·åŒ–: diff_total >= 2ã®å ´åˆã€2å€ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
#   - W_FAIR_TOTAL: 10 â†’ 30ï¼ˆå…¬å¹³æ€§ã®é‡è¦åº¦ã‚’ä¸Šã’ã‚‹ï¼‰
#   - min=2, max=4ã®ã‚ˆã†ãªå·®ãŒå¤§ãã„å ´åˆã«å¼·ãåˆ¶ç´„
# - ä¿®æ­£ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’æ‹¡å……
#   - æœ€é©åŒ–å¾Œã«å…¨ã¦ã®åˆ¶ç´„é•åã‚’å¼·åˆ¶çš„ã«ä¿®æ­£
#   - é †åº: ãƒãƒ¼ãƒ‰åˆ¶ç´„ â†’ TARGET_CAP â†’ 1.2 â†’ BG/HT â†’ gap â†’ å¤–ç—…é™¢DUP â†’ å¤§å­¦3+ â†’ å¤§å­¦å¹³æ—¥åã‚Š â†’ å…¬å¹³æ€§
# v3.2 (2026-01-28):
# - ç”Ÿæˆãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100ã«æˆ»ã™ï¼ˆå‡¦ç†æ™‚é–“ã®æœ€é©åŒ–ï¼‰
#   - NUM_PATTERNS: 10000 â†’ 100
#   - TOP_KEEP: 100 â†’ 20
#   - REFINE_TOP: 20 â†’ 15
# - å¤§å­¦ç—…é™¢2å›ã®å ´åˆã€å¹³æ—¥1å›+ä¼‘æ—¥1å›ã®ãƒãƒ©ãƒ³ã‚¹åˆ¶ç´„ã‚’è¿½åŠ 
#   - bg_weekday_weekend_imbalance é•åã‚’æ¤œå‡º
#   - ãƒšãƒŠãƒ«ãƒ†ã‚£: 50
# - å„ªå…ˆé †ä½ã‚’å³æ ¼åŒ–ï¼ˆTARGET_CAP > gap > DUP ã‚’æ­»å®ˆï¼‰
#   - W_CAP = 200ï¼ˆå„ªå…ˆåº¦1ä½ï¼‰
#   - W_GAP = 100ï¼ˆå„ªå…ˆåº¦2ä½ã€3â†’100ã«å¼·åŒ–ï¼‰
#   - W_EXTERNAL_HOSP_DUP = 70ï¼ˆå„ªå…ˆåº¦3ä½ï¼‰
# v3.1 (2026-01-28):
# - å¤–ç—…é™¢ï¼ˆLï½Yåˆ—ï¼‰é‡è¤‡ã‚’å³æ ¼åŒ–ã€å¤§å­¦ç—…é™¢ï¼ˆBï½Kåˆ—ï¼‰é‡è¤‡ã¯è¨±å®¹
#   - è©•ä¾¡é–¢æ•°ã§å¤–ç—…é™¢é‡è¤‡ã¨å¤§å­¦ç—…é™¢é‡è¤‡ã‚’åˆ†é›¢
#   - W_EXTERNAL_HOSP_DUP=70ï¼ˆå„ªå…ˆåº¦3ä½ï¼šTARGET_CAP > gap > å¤–ç—…é™¢DUPï¼‰
#   - fix_external_hospital_dup_violationsé–¢æ•°ã§æœ€é©åŒ–å¾Œã«å¤–ç—…é™¢é‡è¤‡ã‚’ä¿®æ­£
#   - åŒã˜æ—¥ã®ä»–ã®å¤–ç—…é™¢ã«ç§»å‹•ã€ã¾ãŸã¯å‰²å½“å‰Šé™¤ã§ä¿®æ­£
# v3.0 (2026-01-28):
# - gapé•åï¼ˆ4æ—¥æœªæº€ã®é–“éš”ã§ã®å‰²å½“ï¼‰ã‚’å®Œå…¨ã«æ’é™¤
#   - åˆæœŸãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆæ™‚ã«gapé•å0å€‹ã®å€™è£œã®ã¿é¸æŠ
#   - å±€æ‰€æ¢ç´¢ã§gapé•å1ä»¥ä¸Šã«ãªã‚‹swapã‚’æ‹’å¦
#   - fix_gap_violationsé–¢æ•°ã§æœ€é©åŒ–å¾Œã«gapé•åã‚’å¼·åˆ¶ä¿®æ­£
#   - ç§»å‹•å…ˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å‰²å½“ã‚’å‰Šé™¤ã—ã¦é•åã‚’è§£æ¶ˆ
#   - åŒã˜ç—…é™¢ã ã‘ã§ãªãä»–ã®ç—…é™¢ã®ç©ºãæ ã‚‚æ¢ç´¢
# v2.8 (2026-01-24):
# - å¤§å­¦ç³»ã¨å¤–ç—…é™¢ã®å·®ãŒ3æœªæº€ã«ãªã‚‹åˆ¶ç´„ã‚’è¿½åŠ 
#   - è©•ä¾¡é–¢æ•°ã«å·®ãŒ3ä»¥ä¸Šã®å ´åˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£è¿½åŠ ï¼ˆé‡ã¿100ï¼‰
#   - fix_bg_ht_imbalance_violationsé–¢æ•°ã§æœ€é©åŒ–å¾Œã«å·®3ä»¥ä¸Šã‚’ä¿®æ­£
# - recompute_statsé–¢æ•°ã®BG/HTç¯„å›²ã‚’ä¿®æ­£ï¼ˆBã€œGâ†’Bã€œKã€Hã€œUâ†’Lã€œYï¼‰
#   - ã“ã‚Œã«ã‚ˆã‚Šå‡ºåŠ›Excelã®ä»Šæœˆ/ç´¯è¨ˆã®å¤§å­¦åˆè¨ˆãƒ»å¤–ç—…é™¢åˆè¨ˆãŒæ­£ã—ãè¨ˆç®—ã•ã‚Œã‚‹
# - å¯å¦ã‚³ãƒ¼ãƒ‰1.2ã®åŒ»å¸«ãŒå¤§å­¦ç³»æœ€ä½1å›ã®åˆ¶ç´„ã‚’è¿½åŠ 
#   - get_avail_codeé–¢æ•°ã§1.2ã‚’èªè­˜ã§ãã‚‹ã‚ˆã†ä¿®æ­£
#   - fix_code_1_2_violationsé–¢æ•°ã§æœ€é©åŒ–å¾Œã«å¤§å­¦ç³»0å›ã‚’ä¿®æ­£
#   - è©•ä¾¡é–¢æ•°ã«1.2ã®åŒ»å¸«ãŒå¤§å­¦ç³»0å›ã®å ´åˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£è¿½åŠ ï¼ˆé‡ã¿150ï¼‰
# - TARGET_CAPé•åã®å¼·åˆ¶ä¿®æ­£æ©Ÿèƒ½ã‚’è¿½åŠ 
#   - fix_target_cap_violationsé–¢æ•°ã§æœ€é©åŒ–å¾Œã«TARGET_CAPè¶…éã‚’ä¿®æ­£
#   - ä¸Šä½åŒ»å¸«ï¼ˆå°æ—ã€åŠå·ç­‰ï¼‰ã®å‰²å½“ã‚’ä¸‹ä½åŒ»å¸«ï¼ˆå¤§æ²³å†…ã€çŒªè‚¡ç­‰ï¼‰ã«ç§»å‹•
#   - W_CAPãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’50â†’200ã«å¼·åŒ–
# - ä½™ã‚Šæ ã®å‰²å½“ãƒ­ã‚¸ãƒƒã‚¯ã‚’ä¿®æ­£ï¼ˆæ˜‡é †ã‚½ãƒ¼ãƒˆâ†’æœ€å¾Œã®EXTRA_SLOTSäººã‚’é¸æŠï¼‰
# - ãƒ‡ãƒãƒƒã‚°æƒ…å ±è¿½åŠ ï¼š+1å›å¯¾è±¡ã®åŒ»å¸«åã€TARGET_CAPè¨­å®šå€¤ã€1.2å¯¾è±¡åŒ»å¸«ã‚’è¡¨ç¤º
# - ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã‚’100ã«å¤‰æ›´ï¼ˆç’°å¢ƒå¤‰æ•°ã§ä¸Šæ›¸ãå¯èƒ½ï¼‰
# v2.7 (2026-01-24):
# - ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã®è‡ªå‹•ä¿®æ­£æ©Ÿèƒ½ã‚’å®Ÿè£…
#   - fix_hard_constraint_violationsé–¢æ•°ã‚’è¿½åŠ 
#   - å±€æ‰€æ¢ç´¢å®Œäº†å¾Œã«å…¨ã¦ã®é•åã‚’è‡ªå‹•æ¤œå‡ºãƒ»ä¿®æ­£
#   - ä»£æ›¿åŒ»å¸«ãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯æœªå‰²å½“ã¨ã—ã¦è­¦å‘Šè¡¨ç¤º
#   - ä¿®æ­£å¾Œã«ã‚¹ã‚³ã‚¢ã‚’å†è©•ä¾¡ã—ã¦æœ€çµ‚çµæœã«åæ˜ 
# v2.6 (2026-01-23):
# - get_sched_codeé–¢æ•°ã®é‡å¤§ãªãƒã‚°ã‚’ä¿®æ­£
#   - "0"ã¨"3"ã‚’æœ‰åŠ¹ãªã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ã¨ã—ã¦æ‰±ã‚ãªã„ã‚ˆã†ã«å¤‰æ›´
#   - "0"ã¯ãƒ‡ãƒ¼ã‚¿ãªã—ã€"3"ã¯å¯å¦ã‚³ãƒ¼ãƒ‰ã§ã‚ã‚Šã€ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ã§ã¯ãªã„
#   - ã“ã‚Œã«ã‚ˆã‚Šã€ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«ãŒãã®æ—¥ã«"0"ã‚„"3"ã®å ´åˆã€Lã€œYåˆ—ã¸ã®å‰²å½“ãŒæ­£ã—ãè¨±å¯ã•ã‚Œã‚‹
# v2.5 (2026-01-21):
# - ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ã¨åˆ—ã®åˆ¶ç´„ã‚’ä¿®æ­£
#   - sheet3ã§å°‘ãªãã¨ã‚‚1ã¤ã®ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ï¼ˆA,B,C,CC,D,Eç­‰ï¼‰ã‚’æŒã¤åŒ»å¸«ã‚’ç‰¹å®š
#   - ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«: ãã®æ—¥ã«ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã®ã¿Bã€œKåˆ—å¯ã€ã‚³ãƒ¼ãƒ‰ãŒãªã„æ—¥ã¯å‰²å½“ãªã—
#   - ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«: ãã®æ—¥ã«ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯Lã€œYåˆ—ç¦æ­¢
#   - ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰éä¿æœ‰åŒ»å¸«: Bã€œKåˆ—ã«è‡ªç”±ã«å‰²ã‚Šå½“ã¦å¯èƒ½
# - å‡ºåŠ›ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’1å€‹ã‹ã‚‰3å€‹ã«å¤‰æ›´ï¼ˆTOP3å€™è£œã‚’æç¤ºï¼‰
# v2.4 (2026-01-21):
# - åˆ—æ§‹é€ ã®å¤‰æ›´å¯¾å¿œï¼ˆBã€œYåˆ—ï¼‰
#   - å¯å¦ã‚³ãƒ¼ãƒ‰2: Bã€œQåˆ—ã®ã¿å¯ï¼ˆå¾“æ¥Bã€œMåˆ—ï¼‰
#   - å¯å¦ã‚³ãƒ¼ãƒ‰3: Lã€œYåˆ—ã®ã¿å¯ï¼ˆå¾“æ¥Hã€œUåˆ—ï¼‰
#   - ã‚«ãƒ†è¡¨åˆ¶ç´„: Lã€œYåˆ—ç¦æ­¢ï¼ˆå¾“æ¥Hã€œUåˆ—ï¼‰
# - Bã€œHåˆ—ã®2å›ä¸Šé™åˆ¶ç´„ã‚’å®Ÿè£…
# - è¨ºæ–­ã‚·ãƒ¼ãƒˆã«Bã€œHåˆ—2å›è¶…éé•åæ¤œå‡ºã‚’è¿½åŠ 
# v2.3 (2026-01-21):
# - Bã€œKåˆ—ã®ã‚«ãƒ†è¡¨è¦ä»¶ã‚’ãƒãƒ¼ãƒ‰åˆ¶ç´„ã«å¤‰æ›´ï¼ˆrelax_scheduleã§ç·©å’Œä¸å¯ï¼‰
# - Bã€œKåˆ—ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰æ¬ å¦‚é•åã®æ¤œå‡ºæ©Ÿèƒ½ã‚’è¿½åŠ 
# - can_assign_doc_to_sloté–¢æ•°ã«Bã€œKåˆ—ã‚«ãƒ†è¡¨ãƒã‚§ãƒƒã‚¯ã‚’è¿½åŠ ï¼ˆå±€æ‰€æ¢ç´¢ã§ã‚‚é©ç”¨ï¼‰
# v2.2 (2026-01-21):
# - ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã®ä¿®æ­£ï¼ˆå¯å¦ã‚³ãƒ¼ãƒ‰0ã€ã‚«ãƒ†è¡¨+å¤–ç—…é™¢ã€ã‚³ãƒ¼ãƒ‰2/3é•åï¼‰
# - collect_candidatesé–¢æ•°ã§ã‚³ãƒ¼ãƒ‰0ã‚’å¸¸ã«é™¤å¤–ã™ã‚‹ã‚ˆã†ä¿®æ­£
# - ã‚«ãƒ†è¡¨ãŒã‚ã‚‹æ—¥ã®Hã€œUåˆ—å‰²å½“ã‚’çµ¶å¯¾ç¦æ­¢ã«å¤‰æ›´
# - ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã‚’è¨ºæ–­ã‚·ãƒ¼ãƒˆã«è¿½åŠ 
# v2.1:
# - ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å•é¡Œã®ä¿®æ­£
# - åŒ»å¸«åã®æ­£è¦åŒ–ï¼ˆç©ºç™½é™¤å»ï¼‰
# - sheet4ãƒ˜ãƒƒãƒ€æ¤œå‡ºç¯„å›²ã®æ‹¡å¤§ï¼ˆ30â†’50è¡Œï¼‰
# - date_doc_countã®KeyErrorå¯¾ç­–ï¼ˆdefaultdictåŒ–ï¼‰
# - åŒæ—¥é‡è¤‡ãƒã‚§ãƒƒã‚¯ã®å¼·åŒ–
# - ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°ã®æ”¹å–„

import io
import pandas as pd
import numpy as np
from collections import defaultdict
import random
import importlib.util
import os

COLAB_AVAILABLE = (
    importlib.util.find_spec("google") is not None
    and importlib.util.find_spec("google.colab") is not None
)
if COLAB_AVAILABLE:
    from google.colab import files

# =========================
# ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®š
# =========================
HOLIDAYS = set()  # ç¥æ—¥ã‚’å…¥ã‚Œã‚‹ãªã‚‰ã“ã“ï¼ˆä¾‹: {pd.Timestamp("2026-01-01"), ...}ï¼‰
BG_DAY_COLS = set()    # åˆ—åã§ã€Œæ˜¼ã€å›ºå®šã—ãŸã„å¤§å­¦æ ãŒã‚ã‚Œã°è¿½åŠ 
BG_NIGHT_COLS = set()  # åˆ—åã§ã€Œå¤œã€å›ºå®šã—ãŸã„å¤§å­¦æ ãŒã‚ã‚Œã°è¿½åŠ 

WED_FORBIDDEN_DOCTORS = {'é‡‘åŸ', 'å±±ç”°', 'é‡å¯º'}  # æ°´æ›œã® Hã€œU ã‚’ç¦æ­¢ã—ãŸã„åŒ»å¸«

NUM_PATTERNS = int(os.getenv("NUM_PATTERNS", "100"))  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ100ãƒ‘ã‚¿ãƒ¼ãƒ³

# sheet1 ã®ã€Œæ ã€æ‰±ã„ã™ã‚‹å…¥åŠ›å€¤ï¼ˆ1ä»¥å¤–ã®è¨˜å·ã‚‚è¨±å®¹ã—ãŸã„å ´åˆï¼‰
SLOT_MARKERS = {1, 1.0, "1", "ã€‡", "â—‹", "â—¯", "â—"}

# --- ãƒ­ãƒ¼ã‚«ãƒ«æ¢ç´¢ï¼ˆå…¥æ›¿ï¼‰è¨­å®š ---
LOCAL_SEARCH_ENABLED = True
TOP_KEEP = 20                 # greedyã§æ®‹ã™å€™è£œæ•°ï¼ˆ100ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰ä¸Šä½20å€™è£œã‚’ä¿æŒï¼‰
REFINE_TOP = 15               # ãƒ­ãƒ¼ã‚«ãƒ«æ¢ç´¢ã‚’ã‹ã‘ã‚‹å€™è£œæ•°ï¼ˆä¸Šä½15å€™è£œã‚’æœ€é©åŒ–ï¼‰
LOCAL_MAX_ITERS = 3000        # 1å€™è£œã‚ãŸã‚Šã®å…¥æ›¿è©¦è¡Œå›æ•°
LOCAL_PATIENCE = 1200         # æ”¹å–„ãŒå‡ºãªã„è©¦è¡ŒãŒã“ã®å›æ•°ç¶šã„ãŸã‚‰æ‰“ã¡åˆ‡ã‚Š
LOCAL_REFRESH_EVERY = 200     # å•é¡ŒåŒ»å¸«ï¼ˆgap/é‡è¤‡ï¼‰ã‚’å†æŠ½å‡ºã™ã‚‹é–“éš”

# ã‚¹ã‚³ã‚¢é‡ã¿ï¼ˆå¿…è¦ãªã‚‰èª¿æ•´ï¼‰
# å„ªå…ˆé †ä½: TARGET_CAP > gap > DUP ã‚’æ­»å®ˆ
W_FAIR_TOTAL = 30          # å…¨åˆè¨ˆï¼ˆactiveå†… max-minï¼‰- å…¬å¹³æ€§å¼·åŒ–
W_GAP = 100                # gap(4æ—¥æœªæº€) - å„ªå…ˆåº¦2ä½
W_HOSP_DUP = 1             # åŒä¸€ç—…é™¢è¤‡æ•°å›ï¼ˆå¤§å­¦ç—…é™¢ï¼šè¨±å®¹ï¼‰
W_EXTERNAL_HOSP_DUP = 70   # å¤–ç—…é™¢é‡è¤‡ï¼ˆå³æ ¼ï¼šå„ªå…ˆåº¦3ä½ï¼‰
W_UNASSIGNED = 100         # æœªå‰²å½“
W_CAP = 200                # capè¶…ãˆï¼ˆå³æ ¼åŒ–ï¼šå„ªå…ˆåº¦1ä½ï¼‰
W_BG_SPREAD = 3            # å¤§å­¦åˆè¨ˆï¼ˆç´¯è¨ˆï¼‰ã°ã‚‰ã¤ã
W_HT_SPREAD = 3            # å¤–ç—…é™¢åˆè¨ˆï¼ˆç´¯è¨ˆï¼‰ã°ã‚‰ã¤ã
W_WD_SPREAD = 2            # å¹³æ—¥ï¼ˆç´¯è¨ˆï¼‰ã°ã‚‰ã¤ã
W_WE_SPREAD = 3            # ä¼‘æ—¥åˆè¨ˆï¼ˆç´¯è¨ˆï¼‰ã°ã‚‰ã¤ã
W_BK_LY_BALANCE = 2        # B-K/L-Y ã®æ¯”ç‡ãƒãƒ©ãƒ³ã‚¹ï¼ˆãªã‚‹ã¹ã1:1ï¼‰

# =========================
# ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
# =========================
def strip_cols(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()
    df.columns = [c.strip() if isinstance(c, str) else c for c in df.columns]
    return df

def make_unique(names):
    """é‡è¤‡åˆ—åã‚’ _2, _3 ... ã§ãƒ¦ãƒ‹ãƒ¼ã‚¯åŒ–"""
    seen = {}
    out = []
    for n in names:
        n = "" if pd.isna(n) else n
        if n not in seen:
            seen[n] = 1
            out.append(n)
        else:
            seen[n] += 1
            out.append(f"{n}_{seen[n]}")
    return out

def safe_str(x):
    if pd.isna(x):
        return ""
    return str(x).strip()

# ğŸ”§ FIX: åŒ»å¸«åã®æ­£è¦åŒ–é–¢æ•°ã‚’è¿½åŠ 
def normalize_name(name):
    """åŒ»å¸«åã‚’æ­£è¦åŒ–ï¼ˆå…¨è§’/åŠè§’ã‚¹ãƒšãƒ¼ã‚¹é™¤å»ï¼‰"""
    if pd.isna(name):
        return ""
    return str(name).strip().replace(" ", "").replace("ã€€", "")

def find_sheet_name(xls: pd.ExcelFile, target: str):
    """sheetåã®å¤§å°ãƒ»è¡¨è¨˜ã‚†ã‚Œã«è€ãˆã‚‹"""
    if target in xls.sheet_names:
        return target
    low_map = {s.lower(): s for s in xls.sheet_names}
    if target.lower() in low_map:
        return low_map[target.lower()]
    for s in xls.sheet_names:
        if s.strip().lower() == target.strip().lower():
            return s
    return None

def is_slot_value(v) -> bool:
    if isinstance(v, str):
        return v.strip() in SLOT_MARKERS
    if isinstance(v, (int, float, np.integer, np.floating)):
        return float(v) == 1.0
    return False

# =========================
# sheet4 èª­ã¿è¾¼ã¿ï¼ˆãƒ˜ãƒƒãƒ€è¡Œè‡ªå‹•æ¤œå‡ºï¼‹é‡è¤‡è€æ€§ï¼‰
# ğŸ”§ FIX: æ¤œç´¢ç¯„å›²ã‚’30â†’50è¡Œã«æ‹¡å¤§
# =========================
def parse_sheet4_from_grid(grid: pd.DataFrame) -> pd.DataFrame:
    g = grid.copy()
    g = g.dropna(how="all").reset_index(drop=True)

    if len(g) == 0:
        raise ValueError("sheet4 ãŒç©ºã§ã™")

    # ãƒ˜ãƒƒãƒ€è¡Œã‚’æ¢ã™ï¼ˆ'æ°å' ãŒã‚ã‚‹è¡Œï¼‰
    header_row_idx = None
    search_limit = min(50, len(g))  # ğŸ”§ FIX: 30â†’50ã«æ‹¡å¤§
    for i in range(search_limit):
        row = g.iloc[i].astype(str).str.strip()
        if (row == "æ°å").any():
            header_row_idx = i
            break
    if header_row_idx is None:
        raise ValueError(f"sheet4 ã®ãƒ˜ãƒƒãƒ€è¡Œã« 'æ°å' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆå…ˆé ­{search_limit}è¡Œã‚’æ¤œç´¢ï¼‰")

    headers = [safe_str(x) for x in g.iloc[header_row_idx].tolist()]
    headers = [h if (h != "" and h.lower() != "nan") else f"Unnamed_{j}" for j, h in enumerate(headers)]
    headers = make_unique(headers)

    data = g.iloc[header_row_idx + 1:].reset_index(drop=True)
    data.columns = headers

    if "æ°å" not in data.columns:
        raise ValueError("sheet4 ã®ãƒ˜ãƒƒãƒ€è¡Œã« 'æ°å' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆsheet4ã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")

    # æ°åã®ç©ºè¡Œå‰Šé™¤
    data["æ°å"] = data["æ°å"].astype(str).str.strip()
    data = data[(data["æ°å"].notna()) & (data["æ°å"] != "") & (data["æ°å"].str.lower() != "nan")].reset_index(drop=True)

    # æ•°å€¤åŒ–ï¼ˆæ°åä»¥å¤–ï¼‰
    for col in data.columns:
        if col == "æ°å":
            continue
        data[col] = pd.to_numeric(data[col], errors="coerce").fillna(0)

    return data

# =========================
# å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰
# =========================
print("="*60)
print("   å½“ç›´ãã‚“ v3.4 (TARGET_CAPå³æ ¼åŒ–+å¤šè»¸ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°+å…¬å¹³æ€§å¼·åˆ¶)")
print("="*60)
print("\nsheet1ã€œsheet4ï¼ˆã¾ãŸã¯Sheet4ï¼‰ãŒå…¥ã£ãŸå½“ç›´Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")

if COLAB_AVAILABLE:
    uploaded = files.upload()
    uploaded_filename = list(uploaded.keys())[0]

    try:
        xls = pd.ExcelFile(io.BytesIO(uploaded[uploaded_filename]))
    except Exception as e:
        raise ValueError(f"âŒ Excelãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—ã—ã¾ã—ãŸ: {e}")

    sheet1_name = find_sheet_name(xls, "sheet1")
    sheet2_name = find_sheet_name(xls, "sheet2")
    sheet3_name = find_sheet_name(xls, "sheet3")
    sheet4_name = find_sheet_name(xls, "sheet4") or find_sheet_name(xls, "Sheet4")

    missing = [k for k, v in [("sheet1", sheet1_name), ("sheet2", sheet2_name), ("sheet3", sheet3_name), ("sheet4", sheet4_name)] if v is None]
    if missing:
        raise ValueError(f"âŒ å¿…è¦ãªã‚·ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing}\nå®Ÿéš›ã®ã‚·ãƒ¼ãƒˆå: {xls.sheet_names}")

    # --------- Excel èª­ã¿è¾¼ã¿ ---------
    shift_df = strip_cols(pd.read_excel(xls, sheet_name=sheet1_name))
    availability_raw = strip_cols(pd.read_excel(xls, sheet_name=sheet2_name))
    schedule_raw = strip_cols(pd.read_excel(xls, sheet_name=sheet3_name))

    shift_df.columns = make_unique(list(shift_df.columns))
    availability_raw.columns = make_unique(list(availability_raw.columns))
    schedule_raw.columns = make_unique(list(schedule_raw.columns))

    # sheet4 ã¯ã€Œå‡ºåŠ›ç”¨ã€ã¨ã€Œè§£æç”¨ï¼ˆheader=Noneï¼‰ã€ã‚’åˆ†ã‘ã‚‹
    sheet4_raw_out = strip_cols(pd.read_excel(xls, sheet_name=sheet4_name))
    sheet4_raw_out.columns = make_unique(list(sheet4_raw_out.columns))

    sheet4_grid = pd.read_excel(xls, sheet_name=sheet4_name, header=None)
    sheet4_data = parse_sheet4_from_grid(sheet4_grid)
else:
    from tochoku_data_complete import DATA as LOCAL_DATA

    uploaded_filename = "Tochoku.local.xlsx"
    shift_df = strip_cols(pd.DataFrame(LOCAL_DATA["sheet1"]))
    availability_raw = strip_cols(pd.DataFrame(LOCAL_DATA["sheet2"]))
    schedule_raw = strip_cols(pd.DataFrame(LOCAL_DATA["sheet3"]))

    shift_df.columns = make_unique(list(shift_df.columns))
    availability_raw.columns = make_unique(list(availability_raw.columns))
    schedule_raw.columns = make_unique(list(schedule_raw.columns))

    sheet4_raw_out = strip_cols(pd.DataFrame(LOCAL_DATA["Sheet4"]))
    sheet4_raw_out.columns = make_unique(list(sheet4_raw_out.columns))

    sheet4_data = sheet4_raw_out.copy()
    if "æ°å" not in sheet4_data.columns:
        raise ValueError("âŒ Sheet4 ã® 'æ°å' åˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")
    sheet4_data["æ°å"] = sheet4_data["æ°å"].astype(str).str.strip()
    for col in sheet4_data.columns:
        if col == "æ°å":
            continue
        sheet4_data[col] = pd.to_numeric(sheet4_data[col], errors="coerce").fillna(0)

# =========================
# æ—¥ä»˜åˆ—ã®æ•´å½¢
# ğŸ”§ FIX: ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³å•é¡Œã®ä¿®æ­£
# =========================
date_col_shift = shift_df.columns[0]
shift_df[date_col_shift] = pd.to_datetime(shift_df[date_col_shift], errors="coerce").dt.normalize().dt.tz_localize(None)  # ğŸ”§ FIX
if shift_df[date_col_shift].isna().all():
    raise ValueError("âŒ sheet1 ã®å…ˆé ­åˆ—ãŒæ—¥ä»˜ã¨ã—ã¦è§£é‡ˆã§ãã¾ã›ã‚“ï¼ˆåˆ—ã®å½¢å¼ã‚’ç¢ºèªã—ã¦ãã ã•ã„ï¼‰")

date_col_avail = availability_raw.columns[0]
availability_raw[date_col_avail] = pd.to_datetime(availability_raw[date_col_avail], errors="coerce").dt.normalize().dt.tz_localize(None)  # ğŸ”§ FIX
availability_df = availability_raw.set_index(date_col_avail)

date_col_sched = schedule_raw.columns[0]
schedule_raw[date_col_sched] = pd.to_datetime(schedule_raw[date_col_sched], errors="coerce").dt.normalize().dt.tz_localize(None)  # ğŸ”§ FIX
schedule_df = schedule_raw.set_index(date_col_sched)

# ğŸ”§ FIX: ç¥æ—¥ã‚‚ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³æ­£è¦åŒ–
HOLIDAYS = {pd.to_datetime(d).normalize().tz_localize(None) for d in HOLIDAYS}

def is_holiday(date):
    return pd.to_datetime(date).normalize().tz_localize(None) in HOLIDAYS

# =========================
# åŸºæœ¬æƒ…å ±
# ğŸ”§ FIX: åŒ»å¸«åã‚’æ­£è¦åŒ–
# =========================
doctor_names = [normalize_name(x) for x in list(availability_raw.columns[1:])]  # ğŸ”§ FIX
doctor_col_index = {doc: idx for idx, doc in enumerate(doctor_names)}

# ğŸ”§ FIX: ç¦æ­¢åŒ»å¸«åã‚‚æ­£è¦åŒ–
WED_FORBIDDEN_DOCTORS = {normalize_name(d) for d in WED_FORBIDDEN_DOCTORS}  # ğŸ”§ FIX

hospital_cols = list(shift_df.columns[1:])
n_cols = len(shift_df.columns)

# åˆ—ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ï¼ˆãƒ†ãƒ³ãƒ—ãƒ¬ä¾å­˜ï¼šBã€œY ã‚’æƒ³å®šï¼‰
B_COL_INDEX = 1
C_COL_INDEX = 2
D_COL_INDEX = min(3, n_cols - 1)
E_COL_INDEX = min(4, n_cols - 1)
F_COL_INDEX = min(5, n_cols - 1)
G_COL_INDEX = min(6, n_cols - 1)
H_COL_INDEX = min(7, n_cols - 1)
I_COL_INDEX = min(8, n_cols - 1)
J_COL_INDEX = min(9, n_cols - 1)
K_COL_INDEX = min(10, n_cols - 1)
L_COL_INDEX = min(11, n_cols - 1)
M_COL_INDEX = min(12, n_cols - 1)
Q_COL_INDEX = min(16, n_cols - 1)
U_COL_INDEX = min(20, n_cols - 1)
Y_COL_INDEX = min(24, n_cols - 1)

# åˆ—ç¯„å›²å®šç¾©
B_H_START_INDEX = B_COL_INDEX  # å¤§å­¦ç³»å‰åŠï¼ˆ2å›ã¾ã§ï¼‰
B_H_END_INDEX = H_COL_INDEX
I_K_START_INDEX = I_COL_INDEX  # å¤§å­¦ç³»å¾ŒåŠ
I_K_END_INDEX = K_COL_INDEX
B_K_START_INDEX = B_COL_INDEX  # å¤§å­¦ç³»å…¨ä½“
B_K_END_INDEX = K_COL_INDEX
L_Y_START_INDEX = L_COL_INDEX  # å¤–ç—…é™¢
L_Y_END_INDEX = min(Y_COL_INDEX, n_cols - 1)

print(f"âœ… Excelãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å®Œäº†")
print(f"   åŒ»å¸«æ•°: {len(doctor_names)}äºº")
print(f"   ç—…é™¢åˆ—æ•°: {len(hospital_cols)}åˆ—")
print(f"   å¯¾è±¡æ—¥æ•°: {len(shift_df)}æ—¥")

# =========================
# sheet2 å¯å¦ã‚³ãƒ¼ãƒ‰
# =========================
fallback_avail_codes = {}
for doc in doctor_names:
    col_vals = availability_raw[doc]
    first_val = None
    for v in col_vals:
        if pd.notna(v):
            first_val = v
            break
    if first_val is None:
        fallback_avail_codes[doc] = 1
    else:
        try:
            c = int(first_val)
        except Exception:
            c = 1
        if c not in (0, 1, 2, 3):
            c = 1
        fallback_avail_codes[doc] = c

def get_avail_code(date, doctor):
    code = None
    raw_value = None
    if isinstance(availability_df.index, pd.DatetimeIndex):
        try:
            value = availability_df.at[pd.to_datetime(date).normalize().tz_localize(None), doctor]  # ğŸ”§ FIX
            if isinstance(value, pd.Series):
                value = value.iloc[0]
            if pd.notna(value):
                raw_value = float(value)
                # 1.2ã¯ç‰¹åˆ¥æ‰±ã„ï¼šå¤§å­¦ç³»å„ªå…ˆ
                if abs(raw_value - 1.2) < 0.01:
                    code = 1.2
                else:
                    code = int(raw_value)
        except Exception:
            pass
    if code is None:
        code = fallback_avail_codes.get(doctor, 1)
    if code not in (0, 1, 1.2, 2, 3):
        code = 1
    return code

def get_sched_code(date, doctor):
    """ãã®æ—¥ã®æœ‰åŠ¹ãªã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—ï¼ˆ0ã¨3ã¯ç„¡åŠ¹ã¨ã—ã¦æ‰±ã†ï¼‰"""
    if doctor not in schedule_df.columns:
        return None
    try:
        value = schedule_df.at[pd.to_datetime(date).normalize().tz_localize(None), doctor]  # ğŸ”§ FIX
        if isinstance(value, pd.Series):
            value = value.iloc[0]
    except Exception:
        return None
    if pd.isna(value):
        return None
    code_str = str(value).strip()
    # 0ã¨3ã¯æœ‰åŠ¹ãªã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ã§ã¯ãªã„ï¼ˆ0=ãƒ‡ãƒ¼ã‚¿ãªã—ã€3=å¯å¦ã‚³ãƒ¼ãƒ‰ï¼‰
    if not code_str or code_str == "0" or code_str == "3":
        return None
    return code_str

# sheet2 ã¨ sheet3 ã®åŒ»å¸«åˆ—ãŒã‚ºãƒ¬ã¦ã„ãªã„ã‹ï¼ˆã‚ºãƒ¬ã¦ã¦ã‚‚å‹•ããŒã€åˆ¶ç´„ãŒå¼±ããªã‚‹ï¼‰
sched_doctors = [normalize_name(x) for x in list(schedule_raw.columns[1:])]  # ğŸ”§ FIX
if doctor_names != sched_doctors:
    print("âš ï¸ WARNING: sheet2(å¯å¦) ã¨ sheet3(ã‚«ãƒ†è¡¨) ã®åŒ»å¸«åˆ—ãŒä¸€è‡´ã—ã¦ã„ã¾ã›ã‚“ã€‚")
    only2 = [d for d in doctor_names if d not in sched_doctors]
    only3 = [d for d in sched_doctors if d not in doctor_names]
    if only2:
        print(f"   sheet2 only (å…ˆé ­10): {only2[:10]}")
    if only3:
        print(f"   sheet3 only (å…ˆé ­10): {only3[:10]}")
    print("   â€»Hã€œU ã®ã€ã‚«ãƒ†è¡¨ã‚ã‚Šä¸å¯ã€åˆ¶ç´„ãŒä¸€éƒ¨ã®åŒ»å¸«ã§åŠ¹ã‹ãªã„å¯èƒ½æ€§ãŒã‚ã‚Šã¾ã™ã€‚")

# =========================
# sheet4 å‰æœˆã¾ã§ç´¯ç©
# =========================
name_to_row = {row["æ°å"]: row for _, row in sheet4_data.iterrows()}
prev_names = list(sheet4_data["æ°å"])

def match_prev_name(doc):
    if doc in name_to_row:
        return doc
    ms = [p for p in prev_names if str(p).startswith(doc) or doc.startswith(str(p))]
    return ms[0] if len(ms) == 1 else None

name_match = {doc: match_prev_name(doc) for doc in doctor_names}
unmatched = [d for d in doctor_names if name_match.get(d) is None]
if unmatched:
    print(f"âš ï¸ WARNING: sheet4(ç´¯ç©)ã§åå‰ãŒä¸€è‡´ã—ãªã„åŒ»å¸«ãŒã„ã¾ã™ï¼ˆç´¯ç©ãŒ0æ‰±ã„ã«ãªã‚Šã¾ã™ï¼‰: {unmatched}")

def prev_get(doc, colname):
    pname = name_match.get(doc)
    if pname and pname in name_to_row:
        row = name_to_row[pname]
        v = row.get(colname, 0)
        try:
            return float(v or 0)
        except Exception:
            return 0.0
    return 0.0

prev_total   = {d: prev_get(d, "å…¨åˆè¨ˆ")   for d in doctor_names}
prev_bg      = {d: prev_get(d, "å¤§å­¦åˆè¨ˆ") for d in doctor_names}
prev_ht      = {d: prev_get(d, "å¤–ç—…é™¢åˆè¨ˆ") for d in doctor_names}
prev_weekday = {d: prev_get(d, "å¹³æ—¥")     for d in doctor_names}
prev_weekend = {d: prev_get(d, "ä¼‘æ—¥åˆè¨ˆ") for d in doctor_names}

# =========================
# å…¨æ æ•°ã‚«ã‚¦ãƒ³ãƒˆ + slots_by_date å‰è¨ˆç®—
# =========================
slots_by_date = defaultdict(lambda: {"preassigned": [], "free": []})
preassigned_count = {d: 0 for d in doctor_names}
total_slots = 0

for ridx in shift_df.index:
    date = shift_df.at[ridx, date_col_shift]
    if pd.isna(date):
        continue
    date = pd.to_datetime(date).normalize().tz_localize(None)  # ğŸ”§ FIX

    for hosp in hospital_cols:
        val = shift_df.at[ridx, hosp]

        # å›ºå®šå‰²å½“ï¼ˆã‚»ãƒ«ã«åŒ»å¸«åãŒå…¥ã£ã¦ã„ã‚‹ï¼‰
        val_str = normalize_name(val) if isinstance(val, str) else ""  # ğŸ”§ FIX
        if val_str in doctor_names:
            doc = val_str
            slots_by_date[date]["preassigned"].append((ridx, hosp, doc))
            preassigned_count[doc] += 1
            total_slots += 1
            continue

        # è‡ªå‹•æ ï¼ˆ1/ã€‡ãªã©ï¼‰
        if is_slot_value(val):
            slots_by_date[date]["free"].append((ridx, hosp))
            total_slots += 1

if len(doctor_names) == 0:
    raise ValueError("âŒ sheet2 ã«åŒ»å¸«åãŒã‚ã‚Šã¾ã›ã‚“")

all_dates = sorted(slots_by_date.keys())
all_shift_dates = sorted(pd.to_datetime(shift_df[date_col_shift].dropna()).dt.normalize().dt.tz_localize(None).unique())  # ğŸ”§ FIX

# =========================
# capè¨­è¨ˆï¼šnå›ãƒ™ãƒ¼ã‚¹ï¼‹ä½™ã‚Šã¯å³å´ï¼ˆä¸‹å´ï¼‰ã‹ã‚‰n+1å›
# =========================
def is_always_unavailable(doc):
    if preassigned_count.get(doc, 0) > 0:
        return False
    return all(get_avail_code(d, doc) == 0 for d in all_shift_dates)

inactive_doctors = [d for d in doctor_names if is_always_unavailable(d)]
active_doctors = [d for d in doctor_names if d not in inactive_doctors]
if len(active_doctors) == 0:
    raise ValueError("âŒ å½“æœˆã«å‰²ã‚Šå½“ã¦å¯èƒ½ãªåŒ»å¸«ãŒã„ã¾ã›ã‚“")

BASE_TARGET = total_slots // len(active_doctors)
EXTRA_SLOTS = total_slots - BASE_TARGET * len(active_doctors)

# ä½™ã‚Šæ ã¯å³å´ï¼ˆä¸‹ä½ï¼‰ã®åŒ»å¸«ã«å‰²ã‚Šå½“ã¦ã‚‹
# ä¾‹ï¼šå°æ—(0), åŠå·(1), ..., å¤§æ²³å†…(30), çŒªè‚¡(31) ã®å ´åˆã€å³å´ã®åŒ»å¸«ã‚’é¸æŠ
active_sorted_by_index = sorted(active_doctors, key=lambda d: doctor_col_index[d])  # æ˜‡é †ã‚½ãƒ¼ãƒˆ
EXTRA_ALLOWED = set(active_sorted_by_index[-EXTRA_SLOTS:] if EXTRA_SLOTS > 0 else [])  # æœ€å¾Œã®EXTRA_SLOTSäººï¼ˆå³å´/ä¸‹ä½ï¼‰

TARGET_CAP = {d: 0 for d in doctor_names}
for d in active_doctors:
    TARGET_CAP[d] = BASE_TARGET
for d in EXTRA_ALLOWED:
    TARGET_CAP[d] = BASE_TARGET + 1
for d in doctor_names:
    if preassigned_count.get(d, 0) > TARGET_CAP.get(d, 0):
        TARGET_CAP[d] = preassigned_count[d]

floor_shifts = BASE_TARGET

print(f"\nâœ… å‰²å½“è¨­è¨ˆå®Œäº†")
print(f"   å…¨æ æ•°: {total_slots}")
print(f"   activeåŒ»å¸«: {len(active_doctors)}äºº")
print(f"   inactiveåŒ»å¸«: {len(inactive_doctors)}äºº")
print(f"   åŸºæœ¬å‰²å½“æ•°: {BASE_TARGET}å›")
print(f"   ä½™ã‚Šæ : {EXTRA_SLOTS}æ ï¼ˆå³å´/ä¸‹ä½ã®åŒ»å¸«ã«+1å›ï¼‰")
if EXTRA_ALLOWED:
    extra_docs_display = sorted(EXTRA_ALLOWED, key=lambda d: doctor_col_index[d])
    print(f"   +1å›å¯¾è±¡: {', '.join(extra_docs_display)}")

    # ãƒ‡ãƒãƒƒã‚°ï¼šä¸Šä½åŒ»å¸«ãŒå«ã¾ã‚Œã¦ã„ãªã„ã“ã¨ã‚’ç¢ºèª
    upper_doctors = [d for d in active_doctors if doctor_col_index[d] < 10]  # æœ€åˆã®10äºº
    upper_in_extra = [d for d in upper_doctors if d in EXTRA_ALLOWED]
    if upper_in_extra:
        print(f"   âš ï¸ è­¦å‘Š: ä¸Šä½åŒ»å¸«ãŒ+1å›å¯¾è±¡ã«å«ã¾ã‚Œã¦ã„ã¾ã™: {', '.join(upper_in_extra)}")

    # å„åŒ»å¸«ã®TARGET_CAPã‚’è¡¨ç¤ºï¼ˆæœ€åˆã®5äººã¨æœ€å¾Œã®5äººï¼‰
    cap_display = []
    for d in active_sorted_by_index[:5]:
        cap_display.append(f"{d}={TARGET_CAP[d]}")
    cap_display.append("...")
    for d in active_sorted_by_index[-5:]:
        cap_display.append(f"{d}={TARGET_CAP[d]}")
    print(f"   TARGET_CAP: {' / '.join(cap_display)}")

# =========================
# B-K / L-Y æ¯”ç‡ãƒãƒ©ãƒ³ã‚¹ï¼ˆsheet3ã§ã€Œ3ã€è¨˜è¼‰ã®åŒ»å¸«ã¯é™¤å¤–ï¼‰
# sheet3ã§ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«ã®ç‰¹å®š
# =========================
def has_sheet3_code_3(doc):
    if doc not in schedule_df.columns:
        return False
    values = schedule_df[doc].dropna()
    return any(str(v).strip() == "3" for v in values)

def has_any_schedule_code(doc):
    """åŒ»å¸«ãŒsheet3ã§å°‘ãªãã¨ã‚‚1ã¤ã®ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ï¼ˆA,B,C,CC,D,Eç­‰ã€3ä»¥å¤–ï¼‰ã‚’æŒã£ã¦ã„ã‚‹ã‹"""
    if doc not in schedule_df.columns:
        return False
    values = schedule_df[doc].dropna()
    for v in values:
        s = str(v).strip()
        if s and s != "0" and s != "3":  # 0ã¨3ä»¥å¤–ã®ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚Œã°True
            return True
    return False

RATIO_EXEMPT_DOCTORS = {doc for doc in doctor_names if has_sheet3_code_3(doc)}
if RATIO_EXEMPT_DOCTORS:
    print(f"   æ¯”ç‡ãƒãƒ©ãƒ³ã‚¹é™¤å¤–ï¼ˆsheet3ã«3ã‚ã‚Šï¼‰: {sorted(RATIO_EXEMPT_DOCTORS)}")

SCHEDULE_CODE_HOLDERS = {doc for doc in doctor_names if has_any_schedule_code(doc)}
if SCHEDULE_CODE_HOLDERS:
    print(f"   ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«: {len(SCHEDULE_CODE_HOLDERS)}äºº")
    print(f"   ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰éä¿æœ‰åŒ»å¸«: {len([d for d in doctor_names if d not in SCHEDULE_CODE_HOLDERS])}äºº")

# å¯å¦ã‚³ãƒ¼ãƒ‰1.2ã®åŒ»å¸«ï¼ˆå¤§å­¦ç³»æœ€ä½1å›å¿…é ˆï¼‰
def has_code_1_2(doc):
    """åŒ»å¸«ãŒsheet2ã§å°‘ãªãã¨ã‚‚1ã¤ã®1.2ã‚³ãƒ¼ãƒ‰ã‚’æŒã£ã¦ã„ã‚‹ã‹"""
    if doc not in availability_df.columns:
        return False
    for date in all_shift_dates:
        code = get_avail_code(date, doc)
        if code == 1.2:
            return True
    return False

CODE_1_2_DOCTORS = {doc for doc in doctor_names if has_code_1_2(doc)}
if CODE_1_2_DOCTORS:
    print(f"   å¯å¦ã‚³ãƒ¼ãƒ‰1.2åŒ»å¸«ï¼ˆå¤§å­¦ç³»æœ€ä½1å›å¿…é ˆï¼‰: {len(CODE_1_2_DOCTORS)}äºº")
    print(f"      å¯¾è±¡: {', '.join(sorted(CODE_1_2_DOCTORS)[:10])}")

# =========================
# å¤§å­¦(Bã€œG)ã®æ˜¼å¤œåˆ¤å®š & 7åˆ†é¡
# =========================
def is_bg_day_shift(hosp_name, col_idx):
    if hosp_name in BG_DAY_COLS:
        return True
    if hosp_name in BG_NIGHT_COLS:
        return False
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼šB,C,E,F=æ˜¼ / D,G=å¤œ
    if col_idx in (B_COL_INDEX, C_COL_INDEX, E_COL_INDEX, F_COL_INDEX):
        return True
    if col_idx in (D_COL_INDEX, G_COL_INDEX):
        return False
    mid = (B_COL_INDEX + G_COL_INDEX) // 2
    return col_idx <= mid

def is_bk_slot(col_idx):
    return B_K_START_INDEX <= col_idx <= B_K_END_INDEX

def is_ly_slot(col_idx):
    return L_Y_START_INDEX <= col_idx <= L_Y_END_INDEX

def classify_bg_category(date, hosp_name):
    idx = shift_df.columns.get_loc(hosp_name)
    is_day = is_bg_day_shift(hosp_name, idx)
    dow = pd.to_datetime(date).weekday()
    holi = is_holiday(date)

    weekday = dow < 5
    # å¹³æ—¥ã‹ã¤ C,D,F,G ã¯ç¥æ—¥æ‰±ã„
    if weekday and idx in (C_COL_INDEX, D_COL_INDEX, F_COL_INDEX, G_COL_INDEX):
        holi = True

    if holi and weekday:
        base = "ç¥æ—¥"
    elif dow == 5:
        base = "åœŸæ›œ"
    elif dow == 6:
        base = "æ—¥æ›œ"
    else:
        return "å¹³æ—¥"

    return base + ("æ˜¼" if is_day else "å¤œ")

# =========================
# 1æ ã®åŒ»å¸«é¸æŠï¼ˆgreedyï¼‰
# =========================
def choose_doctor_for_slot(
    date,
    hospital_name,
    assigned_count,
    assigned_dates,
    assigned_bg,
    assigned_ht,
    assigned_weekday,
    assigned_weekend,
    assigned_be,
    assigned_fg,
    assigned_bk,
    assigned_ly,
    assigned_bh,
    assigned_hosp_count,
):
    idx = shift_df.columns.get_loc(hospital_name)
    is_BE = B_COL_INDEX <= idx <= E_COL_INDEX
    is_BG = B_COL_INDEX <= idx <= K_COL_INDEX
    is_BH = B_H_START_INDEX <= idx <= B_H_END_INDEX
    is_LY_range = L_COL_INDEX <= idx <= L_Y_END_INDEX
    is_BK = is_bk_slot(idx)
    is_LY = is_ly_slot(idx)
    dow = pd.to_datetime(date).weekday()
    weekday = dow < 5

    def collect_candidates(
        allow_same_day=False,
        relax_availability=False,
        relax_schedule=False,
        relax_wed=False,
        relax_bh_limit=False,
    ):
        candidates = []
        for doc in doctor_names:
            if not allow_same_day and date in assigned_dates[doc]:
                continue

            code = get_avail_code(date, doc)

            # â˜… ãƒãƒ¼ãƒ‰åˆ¶ç´„1: ã‚³ãƒ¼ãƒ‰0ã¯çµ¶å¯¾ã«ç·©å’Œã—ãªã„
            if code == 0:
                continue

            # ã‚³ãƒ¼ãƒ‰2/3ã®ãƒã‚§ãƒƒã‚¯ï¼ˆrelax_availability=Trueã§ç·©å’Œå¯èƒ½ï¼‰
            if not relax_availability:
                # 2 -> Bã€œQåˆ—ä»¥å¤–ãƒ€ãƒ¡
                if code == 2 and not (B_COL_INDEX <= idx <= Q_COL_INDEX):
                    continue
                # 3 -> Lã€œYåˆ—ä»¥å¤–ãƒ€ãƒ¡
                if code == 3 and not (L_COL_INDEX <= idx <= L_Y_END_INDEX):
                    continue

            # â˜… ãƒãƒ¼ãƒ‰åˆ¶ç´„2: ãã®æ—¥ã«ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ã‚ã‚Šâ†’Lã€œYåˆ—ä¸å¯ï¼ˆçµ¶å¯¾ã«ç·©å’Œã—ãªã„ï¼‰
            if L_COL_INDEX <= idx <= L_Y_END_INDEX:
                if get_sched_code(date, doc):
                    continue

            # â˜… ãƒãƒ¼ãƒ‰åˆ¶ç´„3: Bã€œKåˆ—ã¯ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«ã®ã¿ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦ï¼ˆçµ¶å¯¾ã«ç·©å’Œã—ãªã„ï¼‰
            if B_COL_INDEX <= idx <= B_K_END_INDEX:
                # ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«ã¯ã€ãã®æ—¥ã«ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦
                if doc in SCHEDULE_CODE_HOLDERS and not get_sched_code(date, doc):
                    continue

            # â˜… ãƒãƒ¼ãƒ‰åˆ¶ç´„4: Bã€œHåˆ—ã¯2å›ã¾ã§ï¼ˆrelax_bh_limitã§ç·©å’Œå¯èƒ½ï¼‰
            if not relax_bh_limit and is_BH and assigned_bh[doc] >= 2:
                continue

            # æ°´æ›œæ—¥Lã€œYåˆ—ç¦æ­¢åŒ»å¸«
            if not relax_wed and dow == 2 and is_LY_range:
                if doc in WED_FORBIDDEN_DOCTORS:
                    continue

            if assigned_count[doc] >= TARGET_CAP.get(doc, 0):
                continue

            candidates.append(doc)
        return candidates

    candidates = collect_candidates()
    if not candidates:
        candidates = collect_candidates(allow_same_day=True)
    if not candidates:
        candidates = collect_candidates(allow_same_day=True, relax_availability=True)
    if not candidates:
        candidates = collect_candidates(allow_same_day=True, relax_availability=True, relax_bh_limit=True)
    if not candidates:
        candidates = collect_candidates(
            allow_same_day=True,
            relax_availability=True,
            relax_schedule=True,
            relax_wed=True,
            relax_bh_limit=True,
        )

    if not candidates:
        return None

    any_under_floor = any(assigned_count[d] < floor_shifts for d in active_doctors)
    if any_under_floor:
        under_floor = [d for d in candidates if assigned_count[d] < floor_shifts]
        if under_floor:
            candidates = under_floor

    # gap
    gaps = {}
    for d in candidates:
        if not assigned_dates[d]:
            gaps[d] = 999
        else:
            gaps[d] = min(abs((pd.to_datetime(date) - x).days) for x in assigned_dates[d])

    # å„ªå…ˆé †ä½: 7,4,5,æ¯”ç‡(B-K/L-Y),2,3,6,8,1(>=4),10

    # 7 å…¨ä½“ï¼ˆå‰æœˆ+ä»Šæœˆï¼‰
    metric_total = {d: prev_total[d] + assigned_count[d] for d in candidates}
    min_total = min(metric_total.values())
    candidates = [d for d in candidates if metric_total[d] == min_total]

    # 4 å¤§å­¦/å¤–ç—…é™¢åã‚Šï¼ˆå‰æœˆ+ä»Šæœˆï¼‰
    if is_BG:
        metric_bg = {d: prev_bg[d] + assigned_bg[d] for d in candidates}
        mb = min(metric_bg.values())
        candidates = [d for d in candidates if metric_bg[d] == mb]
    elif is_LY_range:
        metric_ht = {d: prev_ht[d] + assigned_ht[d] for d in candidates}
        mh = min(metric_ht.values())
        candidates = [d for d in candidates if metric_ht[d] == mh]

    # 5 Bã€œE / Fã€œG
    if is_BG:
        if is_BE:
            mbe = min(assigned_be[d] for d in candidates)
            candidates = [d for d in candidates if assigned_be[d] == mbe]
        else:
            mfg = min(assigned_fg[d] for d in candidates)
            candidates = [d for d in candidates if assigned_fg[d] == mfg]

    # B-K / L-Y ã®æ¯”ç‡ãƒãƒ©ãƒ³ã‚¹ï¼ˆé™¤å¤–åŒ»å¸«ä»¥å¤–ï¼‰
    if (is_BK or is_LY) and candidates:
        def imbalance_score(doc):
            if doc in RATIO_EXEMPT_DOCTORS:
                return 0
            bk = assigned_bk[doc] + (1 if is_BK else 0)
            ly = assigned_ly[doc] + (1 if is_LY else 0)
            return abs(bk - ly)

        min_imbalance = min(imbalance_score(d) for d in candidates)
        candidates = [d for d in candidates if imbalance_score(d) == min_imbalance]

    # 2 åŒä¸€ç—…é™¢0å›å„ªå…ˆ
    no_dup = [d for d in candidates if assigned_hosp_count[d].get(hospital_name, 0) == 0]
    if no_dup:
        candidates = no_dup

    # 3 Bã€œG ã¯ã‚«ãƒ†è¡¨ã‚ã‚Šå„ªå…ˆï¼ˆã‚½ãƒ•ãƒˆå„ªå…ˆï¼‰
    if is_BG:
        with_sched = [d for d in candidates if get_sched_code(date, d)]
        if with_sched:
            candidates = with_sched

    # 6 å¹³æ—¥/ä¼‘æ—¥åã‚Šï¼ˆå‰æœˆ+ä»Šæœˆï¼‰
    holi_flag = (
        is_holiday(date)
        or dow >= 5
        or (weekday and idx in (C_COL_INDEX, D_COL_INDEX, F_COL_INDEX, G_COL_INDEX))
    )
    if holi_flag:
        metric_we = {d: prev_weekend[d] + assigned_weekend[d] for d in candidates}
        mwe = min(metric_we.values())
        candidates = [d for d in candidates if metric_we[d] == mwe]
    else:
        metric_wd = {d: prev_weekday[d] + assigned_weekday[d] for d in candidates}
        mwd = min(metric_wd.values())
        candidates = [d for d in candidates if metric_wd[d] == mwd]

    # 8 flooræœªæº€å„ªå…ˆ
    under_floor = [d for d in candidates if assigned_count[d] < floor_shifts]
    if under_floor:
        candidates = under_floor

    # 1 gap>=4
    gap_ok = [d for d in candidates if gaps[d] >= 4]
    if gap_ok:
        candidates = gap_ok

    # 10 åŒç‚¹ãªã‚‰å³å´
    return max(candidates, key=lambda d: doctor_col_index[d])

# =========================
# 1ãƒ¶æœˆåˆ†ç”Ÿæˆï¼ˆgreedyï¼‰
# =========================
def build_schedule_pattern(seed=0):
    random.seed(seed)

    df = shift_df.copy()
    for col in hospital_cols:
        df[col] = df[col].astype(object)

    assigned_count = {d: 0 for d in doctor_names}
    assigned_dates = {d: set() for d in doctor_names}

    assigned_bg = {d: 0 for d in doctor_names}
    assigned_ht = {d: 0 for d in doctor_names}
    assigned_weekday = {d: 0 for d in doctor_names}
    assigned_weekend = {d: 0 for d in doctor_names}
    assigned_be = {d: 0 for d in doctor_names}
    assigned_fg = {d: 0 for d in doctor_names}
    assigned_bk = {d: 0 for d in doctor_names}
    assigned_ly = {d: 0 for d in doctor_names}
    assigned_bh = {d: 0 for d in doctor_names}  # Bã€œHåˆ—ã®å‰²å½“å›æ•°ï¼ˆ2å›ã¾ã§ï¼‰
    assigned_hosp_count = {d: defaultdict(int) for d in doctor_names}
    bg_cat = {d: defaultdict(int) for d in doctor_names}

    # å›ºå®šå½“ç›´
    for date in all_dates:
        for ridx, hosp, doc in slots_by_date[date]["preassigned"]:
            df.at[ridx, hosp] = doc
            assigned_count[doc] += 1
            assigned_dates[doc].add(date)
            assigned_hosp_count[doc][hosp] += 1

            hidx = shift_df.columns.get_loc(hosp)
            if B_COL_INDEX <= hidx <= K_COL_INDEX:
                assigned_bg[doc] += 1
                if B_COL_INDEX <= hidx <= E_COL_INDEX:
                    assigned_be[doc] += 1
                elif F_COL_INDEX <= hidx <= G_COL_INDEX:
                    assigned_fg[doc] += 1
                bg_cat[doc][classify_bg_category(date, hosp)] += 1
            elif L_COL_INDEX <= hidx <= L_Y_END_INDEX:
                assigned_ht[doc] += 1

            # Bã€œHåˆ—ã®ã‚«ã‚¦ãƒ³ãƒˆ
            if B_H_START_INDEX <= hidx <= B_H_END_INDEX:
                assigned_bh[doc] += 1

            dow = date.weekday()
            weekday = dow < 5
            holi_flag = (
                is_holiday(date)
                or dow >= 5
                or (weekday and hidx in (C_COL_INDEX, D_COL_INDEX, F_COL_INDEX, G_COL_INDEX))
            )
            if holi_flag:
                assigned_weekend[doc] += 1
            else:
                assigned_weekday[doc] += 1

            if is_bk_slot(hidx):
                assigned_bk[doc] += 1
            elif is_ly_slot(hidx):
                assigned_ly[doc] += 1

    # è‡ªå‹•å‰²å½“
    for date in all_dates:
        free_slots = slots_by_date[date]["free"].copy()
        random.shuffle(free_slots)

        for ridx, hosp in free_slots:
            chosen = choose_doctor_for_slot(
                date=date,
                hospital_name=hosp,
                assigned_count=assigned_count,
                assigned_dates=assigned_dates,
                assigned_bg=assigned_bg,
                assigned_ht=assigned_ht,
                assigned_weekday=assigned_weekday,
                assigned_weekend=assigned_weekend,
                assigned_be=assigned_be,
                assigned_fg=assigned_fg,
                assigned_bk=assigned_bk,
                assigned_ly=assigned_ly,
                assigned_bh=assigned_bh,
                assigned_hosp_count=assigned_hosp_count,
            )
            if chosen is None:
                remaining = [d for d in doctor_names if assigned_count[d] < TARGET_CAP.get(d, 0)]
                if remaining:
                    fallback_doc = min(remaining, key=lambda d: (assigned_count[d], doctor_col_index[d]))
                else:
                    fallback_doc = min(doctor_names, key=lambda d: (assigned_count[d], doctor_col_index[d]))
                df.at[ridx, hosp] = fallback_doc
                chosen = fallback_doc
            else:
                df.at[ridx, hosp] = chosen

            assigned_count[chosen] += 1
            assigned_dates[chosen].add(date)
            assigned_hosp_count[chosen][hosp] += 1

            hidx = shift_df.columns.get_loc(hosp)
            if B_COL_INDEX <= hidx <= K_COL_INDEX:
                assigned_bg[chosen] += 1
                if B_COL_INDEX <= hidx <= E_COL_INDEX:
                    assigned_be[chosen] += 1
                elif F_COL_INDEX <= hidx <= G_COL_INDEX:
                    assigned_fg[chosen] += 1
                bg_cat[chosen][classify_bg_category(date, hosp)] += 1
            elif L_COL_INDEX <= hidx <= L_Y_END_INDEX:
                assigned_ht[chosen] += 1

            # Bã€œHåˆ—ã®ã‚«ã‚¦ãƒ³ãƒˆ
            if B_H_START_INDEX <= hidx <= B_H_END_INDEX:
                assigned_bh[chosen] += 1

            dow = date.weekday()
            weekday = dow < 5
            holi_flag = (
                is_holiday(date)
                or dow >= 5
                or (weekday and hidx in (C_COL_INDEX, D_COL_INDEX, F_COL_INDEX, G_COL_INDEX))
            )
            if holi_flag:
                assigned_weekend[chosen] += 1
            else:
                assigned_weekday[chosen] += 1

            if is_bk_slot(hidx):
                assigned_bk[chosen] += 1
            elif is_ly_slot(hidx):
                assigned_ly[chosen] += 1

    return (
        df,
        assigned_count,
        assigned_bg,
        assigned_ht,
        assigned_weekday,
        assigned_weekend,
        assigned_bk,
        assigned_ly,
        bg_cat,
    )

# =========================
# slot_meta / movable_positionsï¼ˆãƒ­ãƒ¼ã‚«ãƒ«æ¢ç´¢ç”¨ï¼‰
# =========================
slot_meta = {}  # (ridx,hosp) -> (date, fixed)
movable_positions = []  # (ridx,hosp,date)

for date in all_dates:
    for ridx, hosp, doc in slots_by_date[date]["preassigned"]:
        slot_meta[(ridx, hosp)] = (date, True)
    for ridx, hosp in slots_by_date[date]["free"]:
        slot_meta[(ridx, hosp)] = (date, False)
        movable_positions.append((ridx, hosp, date))

# =========================
# ãƒ‘ã‚¿ãƒ¼ãƒ³çµ±è¨ˆå†è¨ˆç®—ï¼ˆpattern_df ã‹ã‚‰ï¼‰
# =========================
def recompute_stats(pattern_df):
    counts = {d: 0 for d in doctor_names}
    bg_counts = {d: 0 for d in doctor_names}
    ht_counts = {d: 0 for d in doctor_names}
    wd_counts = {d: 0 for d in doctor_names}
    we_counts = {d: 0 for d in doctor_names}
    bk_counts = {d: 0 for d in doctor_names}
    ly_counts = {d: 0 for d in doctor_names}
    bg_cat = {d: defaultdict(int) for d in doctor_names}
    assigned_hosp_count = {d: defaultdict(int) for d in doctor_names}
    doc_assignments = {d: [] for d in doctor_names}  # (date,hosp)
    unassigned = []  # (date,hosp,ridx)

    for (ridx, hosp), (date, fixed) in slot_meta.items():
        val = pattern_df.at[ridx, hosp]
        if not isinstance(val, str):
            continue
        v = normalize_name(val)  # ğŸ”§ FIX

        if v == "UNASSIGNED":
            unassigned.append((date, hosp, ridx))
            continue
        if v not in doctor_names:
            continue

        doc = v
        counts[doc] += 1
        assigned_hosp_count[doc][hosp] += 1
        doc_assignments[doc].append((date, hosp))

        hidx = shift_df.columns.get_loc(hosp)
        # å¤§å­¦ç³»ã¯Bã€œKåˆ—ï¼ˆB_COL_INDEX=1 ã€œ K_COL_INDEX=10ï¼‰
        if B_COL_INDEX <= hidx <= B_K_END_INDEX:
            bg_counts[doc] += 1
            bg_cat[doc][classify_bg_category(date, hosp)] += 1
        # å¤–ç—…é™¢ã¯Lã€œYåˆ—ï¼ˆL_COL_INDEX=11 ã€œ Y_COL_INDEX=24ï¼‰
        elif L_COL_INDEX <= hidx <= L_Y_END_INDEX:
            ht_counts[doc] += 1

        dow = date.weekday()
        weekday = dow < 5
        holi_flag = (
            is_holiday(date)
            or dow >= 5
            or (weekday and hidx in (C_COL_INDEX, D_COL_INDEX, F_COL_INDEX, G_COL_INDEX))
        )
        if holi_flag:
            we_counts[doc] += 1
        else:
            wd_counts[doc] += 1

        if is_bk_slot(hidx):
            bk_counts[doc] += 1
        elif is_ly_slot(hidx):
            ly_counts[doc] += 1

    return (
        counts,
        bg_counts,
        ht_counts,
        wd_counts,
        we_counts,
        bk_counts,
        ly_counts,
        bg_cat,
        assigned_hosp_count,
        doc_assignments,
        unassigned,
    )

# =========================
# ã‚¹ã‚³ã‚¢è©•ä¾¡ï¼ˆraw_scoreã‚‚ä¿æŒã—ã¦ 0 ã§æ½°ã‚Œãªã„ã‚ˆã†ã«ï¼‰
# =========================
def evaluate_schedule_with_raw(
    pattern_df,
    assigned_count,
    assigned_bg,
    assigned_ht,
    assigned_weekday,
    assigned_weekend,
    assigned_bk,
    assigned_ly,
):
    # UNASSIGNED
    unassigned_slots = 0
    for ridx in pattern_df.index:
        for hosp in hospital_cols:
            v = pattern_df.at[ridx, hosp]
            if isinstance(v, str) and normalize_name(v) == "UNASSIGNED":  # ğŸ”§ FIX
                unassigned_slots += 1

    # capé•å
    cap_violations = 0
    for doc in doctor_names:
        cap = TARGET_CAP.get(doc, 0)
        if assigned_count.get(doc, 0) > cap:
            cap_violations += (assigned_count[doc] - cap)

    # å…¨åˆè¨ˆå…¬å¹³æ€§ï¼ˆactiveã®ã¿ï¼‰
    active_counts = [assigned_count.get(d, 0) for d in active_doctors]
    max_c = max(active_counts) if active_counts else 0
    min_c = min(active_counts) if active_counts else 0
    diff_total = max_c - min_c
    # å·®ãŒ2ä»¥ä¸Šã®å ´åˆã€ä¸æº€ãŒé«˜ã„ã®ã§å¼·ã„ãƒšãƒŠãƒ«ãƒ†ã‚£
    # ä¾‹: min=2, max=4ã®å ´åˆã€4å›ã®åŒ»å¸«ã‹ã‚‰2å›ã®åŒ»å¸«ã«æ¸¡ã™ã¹ã
    if diff_total >= 2:
        fairness_penalty = diff_total * 2  # 2å€ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
    else:
        fairness_penalty = max(0, diff_total - 1)

    # gap(4æ—¥æœªæº€) ã¨ åŒä¸€ç—…é™¢é‡è¤‡
    dates_by_doc = defaultdict(list)
    hosp_counts_by_doc = {doc: defaultdict(int) for doc in doctor_names}

    for ridx in pattern_df.index:
        date = pattern_df.at[ridx, date_col_shift]
        if pd.isna(date):
            continue
        date = pd.to_datetime(date).normalize().tz_localize(None)  # ğŸ”§ FIX
        for hosp in hospital_cols:
            val = pattern_df.at[ridx, hosp]
            val_norm = normalize_name(val) if isinstance(val, str) else ""  # ğŸ”§ FIX
            if val_norm in doctor_names:
                dates_by_doc[val_norm].append(date)
                hosp_counts_by_doc[val_norm][hosp] += 1

    gap_violations = 0
    for doc, dlist in dates_by_doc.items():
        dlist = sorted(dlist)
        for i in range(1, len(dlist)):
            if (dlist[i] - dlist[i - 1]).days < 4:
                gap_violations += 1

    hosp_dup_violations = 0
    external_hosp_dup_violations = 0  # å¤–ç—…é™¢é‡è¤‡ï¼ˆå³ã—ãæ‰±ã†ï¼‰
    for doc, hdict in hosp_counts_by_doc.items():
        for hosp, c in hdict.items():
            if c > 1:
                # ç—…é™¢ãŒå¤–ç—…é™¢ï¼ˆLï½Yåˆ—ï¼‰ã‹ã©ã†ã‹ã‚’åˆ¤å®š
                hidx = shift_df.columns.get_loc(hosp)
                if L_COL_INDEX <= hidx <= L_Y_END_INDEX:
                    external_hosp_dup_violations += (c - 1)
                else:
                    hosp_dup_violations += (c - 1)

    # åã‚Šï¼ˆç´¯è¨ˆï¼šå‰æœˆ+ä»Šæœˆï¼‰ã® spread
    bg_vals = [prev_bg[d] + assigned_bg.get(d, 0) for d in active_doctors]
    ht_vals = [prev_ht[d] + assigned_ht.get(d, 0) for d in active_doctors]
    wd_vals = [prev_weekday[d] + assigned_weekday.get(d, 0) for d in active_doctors]
    we_vals = [prev_weekend[d] + assigned_weekend.get(d, 0) for d in active_doctors]

    bg_spread = (max(bg_vals) - min(bg_vals)) if bg_vals else 0
    ht_spread = (max(ht_vals) - min(ht_vals)) if ht_vals else 0
    wd_spread = (max(wd_vals) - min(wd_vals)) if wd_vals else 0
    we_spread = (max(we_vals) - min(we_vals)) if we_vals else 0

    bk_ly_imbalance = 0
    for doc in active_doctors:
        if doc in RATIO_EXEMPT_DOCTORS:
            continue
        bk_val = assigned_bk.get(doc, 0)
        ly_val = assigned_ly.get(doc, 0)
        bk_ly_imbalance += abs(bk_val - ly_val)

    # å¯å¦ã‚³ãƒ¼ãƒ‰1.2ã®åŒ»å¸«ãŒå¤§å­¦ç³»0å›ã®å ´åˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£
    code_1_2_violations = 0
    for doc in CODE_1_2_DOCTORS:
        if assigned_bg.get(doc, 0) == 0:
            code_1_2_violations += 1

    # å¤§å­¦ç³»ã¨å¤–ç—…é™¢ã®å·®ãŒ3ä»¥ä¸Šã®å ´åˆã®ãƒšãƒŠãƒ«ãƒ†ã‚£
    bg_ht_imbalance_violations = 0
    for doc in active_doctors:
        bg = assigned_bg.get(doc, 0)
        ht = assigned_ht.get(doc, 0)
        diff = abs(bg - ht)
        if diff >= 3:
            bg_ht_imbalance_violations += (diff - 2)  # å·®ãŒ3ä»¥ä¸Šã®è¶…éåˆ†ã‚’ã‚«ã‚¦ãƒ³ãƒˆ

    # å¤§å­¦ç—…é™¢2å›ã®å ´åˆã€å¹³æ—¥1å›+ä¼‘æ—¥1å›ã®ãƒãƒ©ãƒ³ã‚¹é•å
    bg_weekday_weekend_imbalance = 0
    bg_over_2_violations = 0  # å¤§å­¦3å›ä»¥ä¸Šã®é•åï¼ˆä¸æº€ãŒé«˜ã„ï¼‰
    bg_weekday_over_violations = 0  # å¤§å­¦ã®å¹³æ—¥åã‚Šï¼ˆå¹³æ—¥2å›ä»¥ä¸Šã¯ä¸æº€ï¼‰
    for doc in active_doctors:
        bg_total = assigned_bg.get(doc, 0)
        weekday_count = bg_cat[doc].get("å¹³æ—¥", 0)

        # å¤§å­¦3å›ä»¥ä¸Šã¯ä¸å¯
        if bg_total >= 3:
            bg_over_2_violations += (bg_total - 2)

        # å¤§å­¦2å›ã®å ´åˆã€å¹³æ—¥1å›+ä¼‘æ—¥1å›ãŒç†æƒ³
        if bg_total == 2:
            if weekday_count == 0 or weekday_count == 2:
                bg_weekday_weekend_imbalance += 1

        # å¤§å­¦ã®å¹³æ—¥ãŒ2å›ä»¥ä¸Šã¯ä¸æº€
        if weekday_count >= 2:
            bg_weekday_over_violations += (weekday_count - 1)

    penalty = 0
    penalty += fairness_penalty * W_FAIR_TOTAL
    penalty += gap_violations * W_GAP
    penalty += hosp_dup_violations * W_HOSP_DUP
    penalty += external_hosp_dup_violations * W_EXTERNAL_HOSP_DUP  # å¤–ç—…é™¢é‡è¤‡ã¯å³æ ¼
    penalty += unassigned_slots * W_UNASSIGNED
    penalty += cap_violations * W_CAP
    penalty += code_1_2_violations * 150  # 1.2ã®åŒ»å¸«ãŒå¤§å­¦ç³»0å›ã®å ´åˆã€å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£
    penalty += bg_ht_imbalance_violations * 100  # å¤§å­¦ç³»ã¨å¤–ç—…é™¢ã®å·®ãŒ3ä»¥ä¸Šã®å ´åˆã€å¤§ããªãƒšãƒŠãƒ«ãƒ†ã‚£
    penalty += bg_weekday_weekend_imbalance * 50  # å¤§å­¦ç—…é™¢2å›ã®å¹³æ—¥/ä¼‘æ—¥ãƒãƒ©ãƒ³ã‚¹é•å
    penalty += bg_over_2_violations * 150  # å¤§å­¦3å›ä»¥ä¸Šã®é•åï¼ˆä¸æº€ãŒé«˜ã„ï¼‰
    penalty += bg_weekday_over_violations * 80  # å¤§å­¦ã®å¹³æ—¥åã‚Šï¼ˆå¹³æ—¥2å›ä»¥ä¸Šã¯ä¸æº€ï¼‰

    penalty += max(0, bg_spread - 1) * W_BG_SPREAD
    penalty += max(0, ht_spread - 1) * W_HT_SPREAD
    penalty += max(0, wd_spread - 1) * W_WD_SPREAD
    penalty += max(0, we_spread - 1) * W_WE_SPREAD
    penalty += bk_ly_imbalance * W_BK_LY_BALANCE

    raw_score = 100 - penalty
    score = max(raw_score, 0)

    metrics = {
        "raw_score": float(raw_score),
        "penalty_total": float(penalty),
        "max_minus_min_total_active": int(diff_total),
        "gap_violations": int(gap_violations),
        "hospital_dup_violations": int(hosp_dup_violations),
        "external_hosp_dup_violations": int(external_hosp_dup_violations),
        "unassigned_slots": int(unassigned_slots),
        "cap_violations": int(cap_violations),
        "code_1_2_violations": int(code_1_2_violations),
        "bg_ht_imbalance_violations": int(bg_ht_imbalance_violations),
        "bg_weekday_weekend_imbalance": int(bg_weekday_weekend_imbalance),
        "bg_over_2_violations": int(bg_over_2_violations),
        "bg_weekday_over_violations": int(bg_weekday_over_violations),
        "bg_spread_cum": float(bg_spread),
        "ht_spread_cum": float(ht_spread),
        "weekday_spread_cum": float(wd_spread),
        "weekend_spread_cum": float(we_spread),
        "bk_ly_imbalance": int(bk_ly_imbalance),
    }
    return score, raw_score, metrics

# =========================
# ãƒ­ãƒ¼ã‚«ãƒ«æ¢ç´¢ï¼ˆå…¥æ›¿ swapï¼‰
# ğŸ”§ FIX: date_doc_count ã‚’å®Œå…¨ãª defaultdict(lambda: defaultdict(int)) ã«å¤‰æ›´
# ğŸ”§ FIX: åŒæ—¥é‡è¤‡ãƒã‚§ãƒƒã‚¯ã®å¼·åŒ–
# =========================
def can_assign_doc_to_slot(doc, date, hosp):
    """ãƒãƒ¼ãƒ‰åˆ¶ç´„ã®ã¿ï¼ˆåŒæ—¥é‡è¤‡ã¯åˆ¥ãƒã‚§ãƒƒã‚¯ï¼‰"""
    idx = shift_df.columns.get_loc(hosp)
    dow = pd.to_datetime(date).weekday()

    code = get_avail_code(date, doc)
    if code == 0:
        return False
    # å¯å¦ã‚³ãƒ¼ãƒ‰2 â†’ Bã€œQåˆ—ã®ã¿å¯
    if code == 2 and not (B_COL_INDEX <= idx <= Q_COL_INDEX):
        return False
    # å¯å¦ã‚³ãƒ¼ãƒ‰3 â†’ Lã€œYåˆ—ã®ã¿å¯
    if code == 3 and not (L_COL_INDEX <= idx <= L_Y_END_INDEX):
        return False
    # ãã®æ—¥ã«ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ã‚ã‚Š â†’ Lã€œYåˆ—ä¸å¯
    if L_COL_INDEX <= idx <= L_Y_END_INDEX:
        if get_sched_code(date, doc):
            return False
    # Bã€œKåˆ—ã¯ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«ã®ã¿ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦
    if B_COL_INDEX <= idx <= B_K_END_INDEX:
        if doc in SCHEDULE_CODE_HOLDERS and not get_sched_code(date, doc):
            return False
    # æ°´æ›œæ—¥Lã€œYåˆ—ç¦æ­¢åŒ»å¸«
    if dow == 2 and L_COL_INDEX <= idx <= L_Y_END_INDEX and doc in WED_FORBIDDEN_DOCTORS:
        return False
    return True

def build_date_doc_count(pattern_df):
    """date -> doc -> countï¼ˆåŒæ—¥è¤‡æ•°å‰²å½“æ¤œå‡ºã‚‚å…¼ã­ã‚‹ï¼‰"""
    # ğŸ”§ FIX: å®Œå…¨ãª nested defaultdict ã«å¤‰æ›´
    date_doc_count = defaultdict(lambda: defaultdict(int))
    for (ridx, hosp), (date, fixed) in slot_meta.items():
        val = pattern_df.at[ridx, hosp]
        if isinstance(val, str):
            v = normalize_name(val)  # ğŸ”§ FIX
            if v in doctor_names:
                date_doc_count[date][v] += 1
    return date_doc_count

def collect_violation_docs_from_assignments(doc_assignments, assigned_hosp_count):
    bad = set()
    # gap
    for doc, assigns in doc_assignments.items():
        dlist = sorted([d for d, _ in assigns])
        for i in range(1, len(dlist)):
            if (dlist[i] - dlist[i - 1]).days < 4:
                bad.add(doc)
                break
    # hospital dup
    for doc, hdict in assigned_hosp_count.items():
        if any(c > 1 for c in hdict.values()):
            bad.add(doc)
    return bad

def is_better_raw(new_raw, new_metrics, cur_raw, cur_metrics):
    if new_raw > cur_raw:
        return True
    if new_raw < cur_raw:
        return False
    # tie-breakï¼ˆé‡è¦åº¦é †ï¼‰
    keys = [
        "unassigned_slots",
        "cap_violations",
        "gap_violations",
        "hospital_dup_violations",
        "max_minus_min_total_active",
        "bk_ly_imbalance",
        "bg_spread_cum",
        "ht_spread_cum",
        "weekday_spread_cum",
        "weekend_spread_cum",
    ]
    return tuple(new_metrics.get(k, 0) for k in keys) < tuple(cur_metrics.get(k, 0) for k in keys)

def local_search_swap(pattern_df, max_iters=2000, patience=800, refresh_every=200, seed=0):
    """å…¥æ›¿ï¼ˆswapï¼‰å±€æ‰€æ¢ç´¢ï¼špreassignedã¯å‹•ã‹ã•ãšã€freeæ ã®ã¿ã‚’å¯¾è±¡ã«æ”¹å–„ã™ã‚‹"""
    if not movable_positions:
        # å‹•ã‹ã›ã‚‹æ ãŒç„¡ã„ï¼ˆå…¨éƒ¨å›ºå®šãªã©ï¼‰
        counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, *_ = recompute_stats(pattern_df)
        score, raw_score, metrics = evaluate_schedule_with_raw(
            pattern_df,
            counts,
            bg_counts,
            ht_counts,
            wd_counts,
            we_counts,
            bk_counts,
            ly_counts,
        )
        return pattern_df.copy(), score, raw_score, metrics

    rng = random.Random(seed)
    df = pattern_df.copy()

    counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)
    cur_score, cur_raw, cur_metrics = evaluate_schedule_with_raw(
        df,
        counts,
        bg_counts,
        ht_counts,
        wd_counts,
        we_counts,
        bk_counts,
        ly_counts,
    )
    date_doc_count = build_date_doc_count(df)

    no_improve = 0
    bad_positions = None

    for it in range(1, max_iters + 1):
        if no_improve >= patience:
            break

        if it == 1 or it % refresh_every == 0:
            bad_docs = collect_violation_docs_from_assignments(doc_assignments, assigned_hosp_count)
            if bad_docs:
                tmp = []
                for (ridx, hosp, date) in movable_positions:
                    v = df.at[ridx, hosp]
                    if isinstance(v, str) and normalize_name(v) in bad_docs:  # ğŸ”§ FIX
                        tmp.append((ridx, hosp, date))
                bad_positions = tmp if tmp else None
            else:
                bad_positions = None

        p1 = rng.choice(bad_positions if bad_positions is not None else movable_positions)
        p2 = rng.choice(movable_positions)
        if p1 == p2:
            no_improve += 1
            continue

        r1, h1, d1 = p1
        r2, h2, d2 = p2

        v1 = df.at[r1, h1]
        v2 = df.at[r2, h2]
        if not (isinstance(v1, str) and isinstance(v2, str)):
            no_improve += 1
            continue

        doc1 = normalize_name(v1)  # ğŸ”§ FIX
        doc2 = normalize_name(v2)  # ğŸ”§ FIX
        if doc1 not in doctor_names or doc2 not in doctor_names:
            no_improve += 1
            continue
        if doc1 == doc2:
            no_improve += 1
            continue

        # ğŸ”§ FIX: åŒæ—¥é‡è¤‡ã‚’ä½œã‚‰ãªã„ï¼ˆåŒã˜æ—¥åŒå£«ã®swapã‚‚ãƒã‚§ãƒƒã‚¯ï¼‰
        if d1 == d2:
            # åŒã˜æ—¥ã®slotåŒå£«ã‚’swapã™ã‚‹å ´åˆã€å…ƒã€…åŒã˜doctorãŒã„ãªã‘ã‚Œã°OK
            # ï¼ˆæ—¢ã«doc1!=doc2ã‚’ç¢ºèªæ¸ˆã¿ãªã®ã§ã€è¿½åŠ ãƒã‚§ãƒƒã‚¯ä¸è¦ï¼‰
            pass
        else:
            # ç•°ãªã‚‹æ—¥ã®swap
            if date_doc_count[d2][doc1] > 0:  # doc1ãŒd2ã«æ—¢ã«ã„ã‚‹
                no_improve += 1
                continue
            if date_doc_count[d1][doc2] > 0:  # doc2ãŒd1ã«æ—¢ã«ã„ã‚‹
                no_improve += 1
                continue

        # ãƒãƒ¼ãƒ‰åˆ¶ç´„
        if not can_assign_doc_to_slot(doc1, d2, h2):
            no_improve += 1
            continue
        if not can_assign_doc_to_slot(doc2, d1, h1):
            no_improve += 1
            continue

        # swapï¼ˆin-placeï¼‰
        df.at[r1, h1], df.at[r2, h2] = doc2, doc1

        # ğŸ”§ FIX: date_doc_count æ›´æ–°ï¼ˆdefaultdictãªã®ã§KeyErrorãªã—ï¼‰
        if d1 != d2:
            date_doc_count[d1][doc1] -= 1
            if date_doc_count[d1][doc1] <= 0:
                del date_doc_count[d1][doc1]
            date_doc_count[d2][doc2] -= 1
            if date_doc_count[d2][doc2] <= 0:
                del date_doc_count[d2][doc2]
            date_doc_count[d1][doc2] += 1
            date_doc_count[d2][doc1] += 1

        # å†è©•ä¾¡ï¼ˆå…¨å†è¨ˆç®—ï¼‰
        counts2, bg2, ht2, wd2, we2, bk2, ly2, bg_cat2, assigned_hosp_count2, doc_assignments2, unassigned2 = recompute_stats(df)
        new_score, new_raw, new_metrics = evaluate_schedule_with_raw(
            df,
            counts2,
            bg2,
            ht2,
            wd2,
            we2,
            bk2,
            ly2,
        )

        # gapé•åãŒ1ä»¥ä¸Šã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ¡ç”¨ã—ãªã„ï¼ˆãƒãƒ¼ãƒ‰åˆ¶ç´„ï¼‰
        new_gap_violations = new_metrics.get("gap_violations", 0)
        if new_gap_violations > 0:
            # revert
            df.at[r1, h1], df.at[r2, h2] = doc1, doc2
            if d1 != d2:
                date_doc_count[d1][doc2] -= 1
                if date_doc_count[d1][doc2] <= 0:
                    del date_doc_count[d1][doc2]
                date_doc_count[d2][doc1] -= 1
                if date_doc_count[d2][doc1] <= 0:
                    del date_doc_count[d2][doc1]
                date_doc_count[d1][doc1] += 1
                date_doc_count[d2][doc2] += 1
            no_improve += 1
        elif is_better_raw(new_raw, new_metrics, cur_raw, cur_metrics):
            cur_score, cur_raw, cur_metrics = new_score, new_raw, new_metrics
            counts, bg_counts, ht_counts, wd_counts, we_counts, bg_cat = counts2, bg2, ht2, wd2, we2, bg_cat2
            assigned_hosp_count, doc_assignments = assigned_hosp_count2, doc_assignments2
            no_improve = 0
        else:
            # revert
            df.at[r1, h1], df.at[r2, h2] = doc1, doc2
            if d1 != d2:
                date_doc_count[d1][doc2] -= 1
                if date_doc_count[d1][doc2] <= 0:
                    del date_doc_count[d1][doc2]
                date_doc_count[d2][doc1] -= 1
                if date_doc_count[d2][doc1] <= 0:
                    del date_doc_count[d2][doc1]
                date_doc_count[d1][doc1] += 1
                date_doc_count[d2][doc2] += 1
            no_improve += 1

    return df, cur_score, cur_raw, cur_metrics

# =========================
# ã‚µãƒãƒªãƒ¼åˆ—ï¼ˆSheet4 ã®åˆ—ã‚’åŸºæº–ã«è‡ªå‹•ç”Ÿæˆï¼‰
# =========================
META_COLS_SHEET4 = {"ã‚«ãƒ†å½“ç•ª", "å‡ºå¼µæ—¥", "å‡ºå¼µå…ˆ"}
BASE_SUMMARY_COLS = {"å…¨åˆè¨ˆ", "å¤§å­¦åˆè¨ˆ", "å¤–ç—…é™¢åˆè¨ˆ", "å¹³æ—¥", "ä¼‘æ—¥åˆè¨ˆ"}

UNIV7_SET = {"å¤§å­¦å¹³æ—¥", "å¤§å­¦åœŸæ›œæ˜¼", "å¤§å­¦åœŸæ›œå¤œ", "å¤§å­¦æ—¥æ›œæ˜¼", "å¤§å­¦æ—¥æ›œå¤œ", "å¤§å­¦ç¥æ—¥æ˜¼", "å¤§å­¦ç¥æ—¥å¤œ"}
UNIV7_ORDER = [c for c in sheet4_raw_out.columns if c in UNIV7_SET]
if not UNIV7_ORDER:
    UNIV7_ORDER = ["å¤§å­¦åœŸæ›œæ˜¼", "å¤§å­¦åœŸæ›œå¤œ", "å¤§å­¦æ—¥æ›œæ˜¼", "å¤§å­¦æ—¥æ›œå¤œ", "å¤§å­¦ç¥æ—¥æ˜¼", "å¤§å­¦ç¥æ—¥å¤œ", "å¤§å­¦å¹³æ—¥"]

DETAIL_COLS = [
    c for c in sheet4_raw_out.columns
    if c not in META_COLS_SHEET4 and c not in ["æ°å"] and c not in BASE_SUMMARY_COLS and c not in UNIV7_SET
]

SUMMARY_DETAIL_COLS = UNIV7_ORDER + DETAIL_COLS
SUMMARY_COLS = ["æ°å", "å…¨åˆè¨ˆ", "å¤§å­¦åˆè¨ˆ", "å¤–ç—…é™¢åˆè¨ˆ", "å¹³æ—¥", "ä¼‘æ—¥åˆè¨ˆ"] + SUMMARY_DETAIL_COLS

def count_doc_in_column(df, colname, doc):
    if colname not in df.columns:
        return 0
    s = df[colname]
    cnt = 0
    for v in s:
        if isinstance(v, str) and normalize_name(v) == doc:  # ğŸ”§ FIX
            cnt += 1
    return cnt

def build_summaries(pattern_df, counts, bg_counts, ht_counts, wd_counts, we_counts, bg_cat_local):
    rows_month = []
    rows_total = []

    uni_map = {
        "å¤§å­¦å¹³æ—¥": "å¹³æ—¥",
        "å¤§å­¦åœŸæ›œæ˜¼": "åœŸæ›œæ˜¼",
        "å¤§å­¦åœŸæ›œå¤œ": "åœŸæ›œå¤œ",
        "å¤§å­¦æ—¥æ›œæ˜¼": "æ—¥æ›œæ˜¼",
        "å¤§å­¦æ—¥æ›œå¤œ": "æ—¥æ›œå¤œ",
        "å¤§å­¦ç¥æ—¥æ˜¼": "ç¥æ—¥æ˜¼",
        "å¤§å­¦ç¥æ—¥å¤œ": "ç¥æ—¥å¤œ",
    }

    for doc in doctor_names:
        pname = name_match.get(doc)
        base_row = name_to_row.get(pname) if pname and pname in name_to_row else None

        def prev_val(colname):
            if base_row is None:
                return 0.0
            v = base_row.get(colname, 0)
            try:
                return float(v or 0)
            except Exception:
                return 0.0

        # ä»Šæœˆ
        row_m = {c: 0.0 for c in SUMMARY_COLS}
        row_m["æ°å"] = doc
        row_m["å…¨åˆè¨ˆ"] = float(counts.get(doc, 0))
        row_m["å¤§å­¦åˆè¨ˆ"] = float(bg_counts.get(doc, 0))
        row_m["å¤–ç—…é™¢åˆè¨ˆ"] = float(ht_counts.get(doc, 0))
        row_m["å¹³æ—¥"] = float(wd_counts.get(doc, 0))
        row_m["ä¼‘æ—¥åˆè¨ˆ"] = float(we_counts.get(doc, 0))

        # å¤§å­¦7åˆ†é¡
        for col in UNIV7_ORDER:
            cat = uni_map.get(col)
            if cat:
                row_m[col] = float(bg_cat_local[doc].get(cat, 0))

        # ç—…é™¢åˆ—ï¼ˆSheet4æº–æ‹ ï¼‰ã‚’ãã®ã¾ã¾æ•°ãˆã‚‹
        for col in DETAIL_COLS:
            row_m[col] = float(count_doc_in_column(pattern_df, col, doc))

        # ç´¯è¨ˆï¼ˆå‰æœˆï¼‹ä»Šæœˆï¼‰
        row_t = {c: 0.0 for c in SUMMARY_COLS}
        row_t["æ°å"] = doc
        for c in SUMMARY_COLS:
            if c == "æ°å":
                continue
            row_t[c] = prev_val(c) + float(row_m.get(c, 0))

        rows_month.append(row_m)
        rows_total.append(row_t)

    return pd.DataFrame(rows_month)[SUMMARY_COLS], pd.DataFrame(rows_total)[SUMMARY_COLS]

# =========================
# è¨ºæ–­ã‚·ãƒ¼ãƒˆç”Ÿæˆï¼ˆåã‚Š & gapé•åä¸€è¦§ï¼‰
# =========================
def build_gap_details(doc_assignments):
    rows = []
    for doc, assigns in doc_assignments.items():
        assigns_sorted = sorted(assigns, key=lambda x: (x[0], x[1]))
        for i in range(1, len(assigns_sorted)):
            d_prev, h_prev = assigns_sorted[i - 1]
            d_cur, h_cur = assigns_sorted[i]
            gap = (d_cur - d_prev).days
            if gap < 4:
                rows.append({
                    "æ°å": doc,
                    "å‰å›æ—¥ä»˜": d_prev,
                    "å‰å›ç—…é™¢": h_prev,
                    "ä»Šå›æ—¥ä»˜": d_cur,
                    "ä»Šå›ç—…é™¢": h_cur,
                    "é–“éš”(æ—¥)": gap,
                })
    cols = ["æ°å", "å‰å›æ—¥ä»˜", "å‰å›ç—…é™¢", "ä»Šå›æ—¥ä»˜", "ä»Šå›ç—…é™¢", "é–“éš”(æ—¥)"]
    if not rows:
        return pd.DataFrame(columns=cols)
    return pd.DataFrame(rows)[cols].sort_values(["æ°å", "ä»Šå›æ—¥ä»˜"]).reset_index(drop=True)

def build_same_day_duplicates(doc_assignments):
    rows = []
    for doc, assigns in doc_assignments.items():
        by_date = defaultdict(list)
        for d, h in assigns:
            by_date[d].append(h)
        for d, hs in by_date.items():
            if len(hs) > 1:
                rows.append({"æ°å": doc, "æ—¥ä»˜": d, "ä»¶æ•°": len(hs), "ç—…é™¢": ", ".join(sorted(hs))})
    cols = ["æ°å", "æ—¥ä»˜", "ä»¶æ•°", "ç—…é™¢"]
    if not rows:
        return pd.DataFrame(columns=cols)
    return pd.DataFrame(rows)[cols].sort_values(["æ—¥ä»˜", "æ°å"]).reset_index(drop=True)

def build_hosp_dup_details(assigned_hosp_count):
    rows = []
    for doc, hdict in assigned_hosp_count.items():
        for hosp, c in hdict.items():
            if c > 1:
                rows.append({"æ°å": doc, "ç—…é™¢": hosp, "å›æ•°": c, "è¶…é": c - 1})
    cols = ["æ°å", "ç—…é™¢", "å›æ•°", "è¶…é"]
    if not rows:
        return pd.DataFrame(columns=cols)
    return pd.DataFrame(rows)[cols].sort_values(["è¶…é", "æ°å"], ascending=[False, True]).reset_index(drop=True)

def build_unassigned_details(unassigned):
    rows = [{"æ—¥ä»˜": d, "ç—…é™¢": hosp, "row_index": ridx} for d, hosp, ridx in unassigned]
    cols = ["æ—¥ä»˜", "ç—…é™¢", "row_index"]
    if not rows:
        return pd.DataFrame(columns=cols)
    return pd.DataFrame(rows)[cols].sort_values(["æ—¥ä»˜", "ç—…é™¢"]).reset_index(drop=True)

def build_doctor_diag(counts, bg_counts, ht_counts, wd_counts, we_counts, doc_assignments, assigned_hosp_count):
    rows = []
    active_set = set(active_doctors)

    for doc in doctor_names:
        assigns = sorted(doc_assignments.get(doc, []), key=lambda x: x[0])
        gaps = [(assigns[i][0] - assigns[i - 1][0]).days for i in range(1, len(assigns))]
        gap_viol = sum(1 for g in gaps if g < 4)
        min_gap = min(gaps) if gaps else None
        hosp_excess = sum(max(0, c - 1) for c in assigned_hosp_count.get(doc, {}).values())

        row = {
            "æ°å": doc,
            "active": 1 if doc in active_set else 0,
            "cap": TARGET_CAP.get(doc, 0),
            "preassigned": preassigned_count.get(doc, 0),

            "ä»Šæœˆ_å…¨åˆè¨ˆ": counts.get(doc, 0),
            "ç´¯è¨ˆ_å…¨åˆè¨ˆ": prev_total.get(doc, 0) + counts.get(doc, 0),

            "ä»Šæœˆ_å¤§å­¦åˆè¨ˆ": bg_counts.get(doc, 0),
            "ç´¯è¨ˆ_å¤§å­¦åˆè¨ˆ": prev_bg.get(doc, 0) + bg_counts.get(doc, 0),

            "ä»Šæœˆ_å¤–ç—…é™¢åˆè¨ˆ": ht_counts.get(doc, 0),
            "ç´¯è¨ˆ_å¤–ç—…é™¢åˆè¨ˆ": prev_ht.get(doc, 0) + ht_counts.get(doc, 0),

            "ä»Šæœˆ_å¹³æ—¥": wd_counts.get(doc, 0),
            "ç´¯è¨ˆ_å¹³æ—¥": prev_weekday.get(doc, 0) + wd_counts.get(doc, 0),

            "ä»Šæœˆ_ä¼‘æ—¥åˆè¨ˆ": we_counts.get(doc, 0),
            "ç´¯è¨ˆ_ä¼‘æ—¥åˆè¨ˆ": prev_weekend.get(doc, 0) + we_counts.get(doc, 0),

            "gapé•åå›æ•°": gap_viol,
            "æœ€å°é–“éš”(æ—¥)": min_gap if min_gap is not None else "",
            "åŒä¸€ç—…é™¢é‡è¤‡è¶…é": hosp_excess,
        }
        rows.append(row)

    df = pd.DataFrame(rows)

    # åã‚Šï¼ˆactiveå¹³å‡ã¨ã®å·®ï¼‰ã‚‚å‡ºã—ã¦ãŠã
    active_rows = df[df["active"] == 1]
    for col in ["ç´¯è¨ˆ_å…¨åˆè¨ˆ", "ç´¯è¨ˆ_å¤§å­¦åˆè¨ˆ", "ç´¯è¨ˆ_å¤–ç—…é™¢åˆè¨ˆ", "ç´¯è¨ˆ_å¹³æ—¥", "ç´¯è¨ˆ_ä¼‘æ—¥åˆè¨ˆ"]:
        if len(active_rows) > 0:
            mean_val = float(active_rows[col].mean())
            df[col + "_å¹³å‡ã¨ã®å·®"] = df[col] - mean_val
        else:
            df[col + "_å¹³å‡ã¨ã®å·®"] = 0.0

    return df.sort_values(["active", "ç´¯è¨ˆ_å…¨åˆè¨ˆ"], ascending=[False, False]).reset_index(drop=True)

def build_metrics_df(score_clamped, raw_score, metrics):
    row = {"score": float(score_clamped), "raw_score": float(raw_score), **metrics}
    return pd.DataFrame([row])

def build_hard_constraint_violations(pattern_df):
    """ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã®è©³ç´°ãƒªã‚¹ãƒˆã‚’ç”Ÿæˆ"""
    rows = []

    for ridx in pattern_df.index:
        date = pattern_df.at[ridx, date_col_shift]
        if pd.isna(date):
            continue
        date = pd.to_datetime(date).normalize().tz_localize(None)
        dow = date.weekday()

        for hosp in hospital_cols:
            val = pattern_df.at[ridx, hosp]
            if not isinstance(val, str):
                continue
            doc = normalize_name(val)
            if doc not in doctor_names:
                continue

            idx = shift_df.columns.get_loc(hosp)
            code = get_avail_code(date, doc)
            sched_code = get_sched_code(date, doc)

            # é•å1: å¯å¦ã‚³ãƒ¼ãƒ‰0
            if code == 0:
                rows.append({
                    "é•åç¨®åˆ¥": "å¯å¦ã‚³ãƒ¼ãƒ‰0é•å",
                    "æ—¥ä»˜": date,
                    "åŒ»å¸«å": doc,
                    "ç—…é™¢": hosp,
                    "åˆ—ç•ªå·": idx,
                    "å¯å¦ã‚³ãƒ¼ãƒ‰": code,
                    "ã‚«ãƒ†è¡¨": sched_code if sched_code else "",
                    "è©³ç´°": "ã‚³ãƒ¼ãƒ‰0ï¼ˆä¸å¯ï¼‰ã®æ—¥ã«å‰²å½“",
                })

            # é•å2: å¯å¦ã‚³ãƒ¼ãƒ‰2é•åï¼ˆQåˆ—ã‚ˆã‚Šå¾Œã«å‰²å½“ï¼‰
            if code == 2 and not (B_COL_INDEX <= idx <= Q_COL_INDEX):
                rows.append({
                    "é•åç¨®åˆ¥": "å¯å¦ã‚³ãƒ¼ãƒ‰2é•å",
                    "æ—¥ä»˜": date,
                    "åŒ»å¸«å": doc,
                    "ç—…é™¢": hosp,
                    "åˆ—ç•ªå·": idx,
                    "å¯å¦ã‚³ãƒ¼ãƒ‰": code,
                    "ã‚«ãƒ†è¡¨": sched_code if sched_code else "",
                    "è©³ç´°": f"ã‚³ãƒ¼ãƒ‰2ã¯Bã€œQåˆ—ã®ã¿å¯ã€‚åˆ—{idx}ã«å‰²å½“",
                })

            # é•å3: å¯å¦ã‚³ãƒ¼ãƒ‰3é•åï¼ˆLã€œYåˆ—ä»¥å¤–ã«å‰²å½“ï¼‰
            if code == 3 and not (L_COL_INDEX <= idx <= L_Y_END_INDEX):
                rows.append({
                    "é•åç¨®åˆ¥": "å¯å¦ã‚³ãƒ¼ãƒ‰3é•å",
                    "æ—¥ä»˜": date,
                    "åŒ»å¸«å": doc,
                    "ç—…é™¢": hosp,
                    "åˆ—ç•ªå·": idx,
                    "å¯å¦ã‚³ãƒ¼ãƒ‰": code,
                    "ã‚«ãƒ†è¡¨": sched_code if sched_code else "",
                    "è©³ç´°": f"ã‚³ãƒ¼ãƒ‰3ã¯Lã€œYåˆ—ã®ã¿å¯ã€‚åˆ—{idx}ã«å‰²å½“",
                })

            # é•å4: ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ã‚ã‚Šï¼‹Lã€œYåˆ—é•å
            if L_COL_INDEX <= idx <= L_Y_END_INDEX and sched_code:
                rows.append({
                    "é•åç¨®åˆ¥": "ã‚«ãƒ†è¡¨+å¤–ç—…é™¢é•å",
                    "æ—¥ä»˜": date,
                    "åŒ»å¸«å": doc,
                    "ç—…é™¢": hosp,
                    "åˆ—ç•ªå·": idx,
                    "å¯å¦ã‚³ãƒ¼ãƒ‰": code,
                    "ã‚«ãƒ†è¡¨": sched_code,
                    "è©³ç´°": f"ã‚«ãƒ†è¡¨ï¼ˆ{sched_code}ï¼‰ãŒã‚ã‚‹æ—¥ã¯å¤–ç—…é™¢ï¼ˆLã€œYåˆ—ï¼‰ã«å‰²å½“ä¸å¯ã€‚åˆ—{idx}ã«å‰²å½“",
                })

            # é•å5: Bã€œKåˆ—ã§ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ãªã—ï¼ˆã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«ã®ã¿ï¼‰
            if B_COL_INDEX <= idx <= B_K_END_INDEX and doc in SCHEDULE_CODE_HOLDERS and not sched_code:
                rows.append({
                    "é•åç¨®åˆ¥": "B-Kåˆ—ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰æ¬ å¦‚",
                    "æ—¥ä»˜": date,
                    "åŒ»å¸«å": doc,
                    "ç—…é™¢": hosp,
                    "åˆ—ç•ªå·": idx,
                    "å¯å¦ã‚³ãƒ¼ãƒ‰": code,
                    "ã‚«ãƒ†è¡¨": "",
                    "è©³ç´°": f"Bã€œKåˆ—ï¼ˆå¤§å­¦ç³»ï¼‰ã®å‰²å½“ã«ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ãŒå¿…è¦ï¼ˆã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«ï¼‰ã€‚åˆ—{idx}ã«å‰²å½“",
                })

            # é•å6: æ°´æ›œæ—¥Lã€œYåˆ—ç¦æ­¢åŒ»å¸«
            if dow == 2 and L_COL_INDEX <= idx <= L_Y_END_INDEX and doc in WED_FORBIDDEN_DOCTORS:
                rows.append({
                    "é•åç¨®åˆ¥": "æ°´æ›œæ—¥Lã€œYåˆ—ç¦æ­¢é•å",
                    "æ—¥ä»˜": date,
                    "åŒ»å¸«å": doc,
                    "ç—…é™¢": hosp,
                    "åˆ—ç•ªå·": idx,
                    "å¯å¦ã‚³ãƒ¼ãƒ‰": code,
                    "ã‚«ãƒ†è¡¨": sched_code if sched_code else "",
                    "è©³ç´°": f"{doc}ã¯æ°´æ›œæ—¥ã®Lã€œYåˆ—ç¦æ­¢",
                })

    # Bã€œHåˆ—ã®2å›è¶…éé•åã‚’ãƒã‚§ãƒƒã‚¯
    bh_counts = defaultdict(list)
    for ridx in pattern_df.index:
        date = pattern_df.at[ridx, date_col_shift]
        if pd.isna(date):
            continue
        date = pd.to_datetime(date).normalize().tz_localize(None)

        for hosp in hospital_cols:
            val = pattern_df.at[ridx, hosp]
            if not isinstance(val, str):
                continue
            doc = normalize_name(val)
            if doc not in doctor_names:
                continue

            idx = shift_df.columns.get_loc(hosp)
            if B_H_START_INDEX <= idx <= B_H_END_INDEX:
                bh_counts[doc].append((date, hosp, idx))

    # é•å7: Bã€œHåˆ—ãŒ2å›è¶…é
    for doc, assignments in bh_counts.items():
        if len(assignments) > 2:
            for date, hosp, idx in assignments[2:]:  # 3å›ç›®ä»¥é™
                code = get_avail_code(date, doc)
                sched_code = get_sched_code(date, doc)
                rows.append({
                    "é•åç¨®åˆ¥": "B-Håˆ—2å›è¶…éé•å",
                    "æ—¥ä»˜": date,
                    "åŒ»å¸«å": doc,
                    "ç—…é™¢": hosp,
                    "åˆ—ç•ªå·": idx,
                    "å¯å¦ã‚³ãƒ¼ãƒ‰": code,
                    "ã‚«ãƒ†è¡¨": sched_code if sched_code else "",
                    "è©³ç´°": f"Bã€œHåˆ—ã¯2å›ã¾ã§ã€‚{len(assignments)}å›ç›®ã®å‰²å½“",
                })

    cols = ["é•åç¨®åˆ¥", "æ—¥ä»˜", "åŒ»å¸«å", "ç—…é™¢", "åˆ—ç•ªå·", "å¯å¦ã‚³ãƒ¼ãƒ‰", "ã‚«ãƒ†è¡¨", "è©³ç´°"]
    if not rows:
        return pd.DataFrame(columns=cols)
    return pd.DataFrame(rows)[cols].sort_values(["é•åç¨®åˆ¥", "æ—¥ä»˜", "åŒ»å¸«å"]).reset_index(drop=True)

def fix_hard_constraint_violations(pattern_df, max_attempts=50, verbose=True):
    """
    ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã‚’è‡ªå‹•ä¿®æ­£ã™ã‚‹

    Args:
        pattern_df: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DataFrame
        max_attempts: æœ€å¤§è©¦è¡Œå›æ•°
        verbose: ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã‹

    Returns:
        (ä¿®æ­£å¾Œã®DataFrame, æˆåŠŸãƒ•ãƒ©ã‚°, ä¿®æ­£æ•°, ä¿®æ­£å¤±æ•—æ•°)
    """
    df = pattern_df.copy()
    total_fixed = 0
    total_failed = 0

    for attempt in range(max_attempts):
        violations_df = build_hard_constraint_violations(df)

        if len(violations_df) == 0:
            if verbose and total_fixed > 0:
                print(f"   âœ… ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã‚’{total_fixed}ä»¶ä¿®æ­£ã—ã¾ã—ãŸ")
            return df, True, total_fixed, total_failed

        if attempt == 0 and verbose:
            print(f"   âš ï¸ ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã‚’{len(violations_df)}ä»¶æ¤œå‡º â†’ è‡ªå‹•ä¿®æ­£ã‚’é–‹å§‹...")

        # å„é•åã‚’ä¿®æ­£è©¦è¡Œ
        fixed_in_this_iteration = 0

        for _, violation in violations_df.iterrows():
            date = violation['æ—¥ä»˜']
            doc = violation['åŒ»å¸«å']
            hosp = violation['ç—…é™¢']
            violation_type = violation['é•åç¨®åˆ¥']

            # è©²å½“è¡Œã‚’æ¢ã™
            ridx = None
            for idx in df.index:
                if pd.to_datetime(df.at[idx, date_col_shift]).normalize().tz_localize(None) == date:
                    ridx = idx
                    break

            if ridx is None:
                continue

            # é•åã—ã¦ã„ã‚‹å‰²å½“ã‚’è§£é™¤
            current_val = df.at[ridx, hosp]
            if not isinstance(current_val, str) or normalize_name(current_val) != doc:
                continue

            df.at[ridx, hosp] = None

            # ä»£æ›¿åŒ»å¸«ã‚’æ¢ã™
            col_idx = shift_df.columns.get_loc(hosp)
            dow = pd.to_datetime(date).weekday()

            # ã“ã®æ—¥ã«æ—¢ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ã‚‹åŒ»å¸«ã‚’é™¤å¤–
            already_assigned_on_date = set()
            for h in hospital_cols:
                v = df.at[ridx, h]
                if isinstance(v, str):
                    already_assigned_on_date.add(normalize_name(v))

            # å€™è£œåŒ»å¸«ã‚’æ¢ã™ï¼ˆãƒãƒ¼ãƒ‰åˆ¶ç´„ã®ã¿ãƒã‚§ãƒƒã‚¯ï¼‰
            candidates = []
            for candidate_doc in doctor_names:
                # åŒæ—¥é‡è¤‡ãƒã‚§ãƒƒã‚¯
                if candidate_doc in already_assigned_on_date:
                    continue

                # ãƒãƒ¼ãƒ‰åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
                if can_assign_doc_to_slot(candidate_doc, date, hosp):
                    candidates.append(candidate_doc)

            if candidates:
                # å„ªå…ˆé †ä½ï¼šå…¨ä½“åˆè¨ˆãŒå°‘ãªã„åŒ»å¸«ã‚’å„ªå…ˆ
                candidates.sort(key=lambda d: prev_total.get(d, 0) + len([1 for h in hospital_cols for ridx2 in df.index if isinstance(df.at[ridx2, h], str) and normalize_name(df.at[ridx2, h]) == d]))
                new_doc = candidates[0]
                df.at[ridx, hosp] = new_doc
                fixed_in_this_iteration += 1
                total_fixed += 1
            else:
                # ä»£æ›¿åŒ»å¸«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ â†’ æœªå‰²å½“ã®ã¾ã¾
                total_failed += 1
                if verbose:
                    print(f"   âš ï¸ ä¿®æ­£å¤±æ•—: {date.strftime('%Y-%m-%d')} {hosp} ({violation_type})")

        # é€²æ—ãŒãªã‘ã‚Œã°ãƒ«ãƒ¼ãƒ—çµ‚äº†
        # ä¿®æ­£ãŒé€²ã¾ãªãã¦ã‚‚max_attemptsã¾ã§è©¦è¡Œã‚’ç¶šã‘ã‚‹
        # if fixed_in_this_iteration == 0:
        #     break

    # æœ€çµ‚ãƒã‚§ãƒƒã‚¯
    final_violations = build_hard_constraint_violations(df)
    success = len(final_violations) == 0

    if verbose:
        if success:
            print(f"   âœ… å…¨ã¦ã®ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")
        else:
            print(f"   âš ï¸ {len(final_violations)}ä»¶ã®ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åãŒæ®‹ã£ã¦ã„ã¾ã™ï¼ˆä¿®æ­£æ•°: {total_fixed}, å¤±æ•—: {total_failed}ï¼‰")

    return df, success, total_fixed, total_failed

def fix_target_cap_violations(pattern_df, max_attempts=100, verbose=True):
    """
    TARGET_CAPé•åã‚’ä¿®æ­£ã™ã‚‹ï¼ˆä¸Šä½åŒ»å¸«ãŒä¸‹ä½åŒ»å¸«ã‚ˆã‚Šå¤šããªã‚‰ãªã„ã‚ˆã†å¼·åˆ¶ï¼‰

    Args:
        pattern_df: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DataFrame
        max_attempts: æœ€å¤§è©¦è¡Œå›æ•°
        verbose: ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã‹

    Returns:
        (ä¿®æ­£å¾Œã®DataFrame, æˆåŠŸãƒ•ãƒ©ã‚°, ä¿®æ­£æ•°)
    """
    df = pattern_df.copy()
    total_fixed = 0

    for attempt in range(max_attempts):
        # ç¾åœ¨ã®å‰²å½“å›æ•°ã‚’å†è¨ˆç®—
        counts, *_ = recompute_stats(df)

        # capè¶…éã—ã¦ã„ã‚‹åŒ»å¸«ã‚’ç‰¹å®š
        over_cap_docs = []
        under_cap_docs = []

        for doc in active_doctors:
            current = counts.get(doc, 0)
            cap = TARGET_CAP.get(doc, 0)

            if current > cap:
                over_cap_docs.append((doc, current - cap))
            elif current < cap:
                under_cap_docs.append((doc, cap - current))

        if not over_cap_docs:
            if verbose and total_fixed > 0:
                print(f"   âœ… TARGET_CAPé•åã‚’{total_fixed}ä»¶ä¿®æ­£ã—ã¾ã—ãŸ")
            return df, True, total_fixed

        if attempt == 0 and verbose:
            over_cap_names = [f"{doc}({counts[doc]}/{TARGET_CAP[doc]})" for doc, _ in over_cap_docs]
            print(f"   âš ï¸ TARGET_CAPè¶…éã‚’æ¤œå‡º â†’ è‡ªå‹•ä¿®æ­£ã‚’é–‹å§‹...")
            print(f"      è¶…é: {', '.join(over_cap_names[:5])}")

        # ä¿®æ­£è©¦è¡Œ
        fixed_in_this_iteration = 0

        for over_doc, excess in over_cap_docs:
            if excess <= 0:
                continue

            # ã“ã®over_docã®å‰²å½“ã‚’æ¢ã™
            over_doc_positions = []
            for ridx in df.index:
                date = df.at[ridx, date_col_shift]
                if pd.isna(date):
                    continue
                date = pd.to_datetime(date).normalize().tz_localize(None)

                for hosp in hospital_cols:
                    val = df.at[ridx, hosp]
                    if isinstance(val, str) and normalize_name(val) == over_doc:
                        over_doc_positions.append((ridx, hosp, date))

            # ãƒ©ãƒ³ãƒ€ãƒ ã«1ã¤é¸ã‚“ã§ç§»å‹•ã‚’è©¦ã¿ã‚‹
            import random
            random.shuffle(over_doc_positions)

            for ridx, hosp, date in over_doc_positions[:min(excess, 3)]:  # æœ€å¤§3å€‹ã¾ã§è©¦è¡Œ
                # ã“ã®æ—¥ã«æ—¢ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ã‚‹åŒ»å¸«ã‚’é™¤å¤–
                already_assigned_on_date = set()
                for h in hospital_cols:
                    v = df.at[ridx, h]
                    if isinstance(v, str):
                        already_assigned_on_date.add(normalize_name(v))

                # under_capã®åŒ»å¸«ã®ä¸­ã‹ã‚‰ä»£æ›¿ã‚’æ¢ã™
                candidates = []
                for under_doc, deficit in under_cap_docs:
                    if deficit <= 0:
                        continue
                    if under_doc in already_assigned_on_date:
                        continue
                    if can_assign_doc_to_slot(under_doc, date, hosp):
                        candidates.append(under_doc)

                if candidates:
                    # å…¨ä½“åˆè¨ˆãŒå°‘ãªã„åŒ»å¸«ã‚’å„ªå…ˆ
                    candidates.sort(key=lambda d: prev_total.get(d, 0) + counts.get(d, 0))
                    new_doc = candidates[0]

                    # å…¥ã‚Œæ›¿ãˆ
                    df.at[ridx, hosp] = new_doc
                    fixed_in_this_iteration += 1
                    total_fixed += 1

                    # under_cap_docsã‚’æ›´æ–°
                    for i, (d, deficit) in enumerate(under_cap_docs):
                        if d == new_doc:
                            under_cap_docs[i] = (d, deficit - 1)
                            break

                    break  # æ¬¡ã®over_docã¸

        # ä¿®æ­£ãŒé€²ã¾ãªãã¦ã‚‚max_attemptsã¾ã§è©¦è¡Œã‚’ç¶šã‘ã‚‹
        # if fixed_in_this_iteration == 0:
        #     break

    # æœ€çµ‚ç¢ºèª
    counts, *_ = recompute_stats(df)
    remaining_violations = sum(1 for doc in active_doctors if counts.get(doc, 0) > TARGET_CAP.get(doc, 0))

    if verbose:
        if remaining_violations == 0:
            print(f"   âœ… å…¨ã¦ã®TARGET_CAPé•åã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")
        else:
            print(f"   âš ï¸ {remaining_violations}ä»¶ã®TARGET_CAPé•åãŒæ®‹ã£ã¦ã„ã¾ã™ï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")

    return df, remaining_violations == 0, total_fixed

def fix_code_1_2_violations(pattern_df, max_attempts=100, verbose=True):
    """
    å¯å¦ã‚³ãƒ¼ãƒ‰1.2ã®åŒ»å¸«ãŒå¤§å­¦ç³»0å›ã®é•åã‚’ä¿®æ­£ã™ã‚‹

    Args:
        pattern_df: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DataFrame
        max_attempts: æœ€å¤§è©¦è¡Œå›æ•°
        verbose: ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã‹

    Returns:
        (ä¿®æ­£å¾Œã®DataFrame, æˆåŠŸãƒ•ãƒ©ã‚°, ä¿®æ­£æ•°)
    """
    if not CODE_1_2_DOCTORS:
        return pattern_df, True, 0

    df = pattern_df.copy()
    total_fixed = 0

    for attempt in range(max_attempts):
        # ç¾åœ¨ã®å‰²å½“å›æ•°ã‚’å†è¨ˆç®—
        counts, bg_counts, *_ = recompute_stats(df)

        # å¤§å­¦ç³»0å›ã®1.2åŒ»å¸«ã‚’ç‰¹å®š
        zero_bg_docs = []
        for doc in CODE_1_2_DOCTORS:
            if bg_counts.get(doc, 0) == 0:
                zero_bg_docs.append(doc)

        if not zero_bg_docs:
            if verbose and total_fixed > 0:
                print(f"   âœ… å¯å¦ã‚³ãƒ¼ãƒ‰1.2åŒ»å¸«ã®å¤§å­¦ç³»0å›é•åã‚’{total_fixed}ä»¶ä¿®æ­£ã—ã¾ã—ãŸ")
            return df, True, total_fixed

        if attempt == 0 and verbose:
            print(f"   âš ï¸ å¯å¦ã‚³ãƒ¼ãƒ‰1.2åŒ»å¸«ã®å¤§å­¦ç³»0å›é•åã‚’æ¤œå‡º â†’ è‡ªå‹•ä¿®æ­£ã‚’é–‹å§‹...")
            print(f"      å¯¾è±¡: {', '.join(zero_bg_docs[:5])}")

        # ä¿®æ­£è©¦è¡Œ
        fixed_in_this_iteration = 0

        for zero_doc in zero_bg_docs:
            # ã“ã®zero_docã®å¤–ç—…é™¢ï¼ˆLã€œYï¼‰å‰²å½“ã‚’æ¢ã™
            zero_doc_ly_positions = []
            for ridx in df.index:
                date = df.at[ridx, date_col_shift]
                if pd.isna(date):
                    continue
                date = pd.to_datetime(date).normalize().tz_localize(None)

                for hosp in hospital_cols:
                    idx = shift_df.columns.get_loc(hosp)
                    # Lã€œYåˆ—ï¼ˆå¤–ç—…é™¢ï¼‰ã®ã¿
                    if L_COL_INDEX <= idx <= L_Y_END_INDEX:
                        val = df.at[ridx, hosp]
                        if isinstance(val, str) and normalize_name(val) == zero_doc:
                            zero_doc_ly_positions.append((ridx, hosp, date))

            # å¤–ç—…é™¢ã®å‰²å½“ã‚’1ã¤å¤§å­¦ç³»ã«å¤‰æ›´
            import random
            random.shuffle(zero_doc_ly_positions)

            for ridx, hosp, date in zero_doc_ly_positions[:1]:  # 1ã¤ã ã‘è©¦è¡Œ
                # ã“ã®æ—¥ä»˜ã®Bã€œKåˆ—ï¼ˆå¤§å­¦ç³»ï¼‰ã§ç©ºã„ã¦ã„ã‚‹æ ã‚’æ¢ã™
                for bg_hosp in hospital_cols:
                    bg_idx = shift_df.columns.get_loc(bg_hosp)
                    if not (B_COL_INDEX <= bg_idx <= B_K_END_INDEX):
                        continue

                    val = df.at[ridx, bg_hosp]
                    # ç©ºãæ ã‹ã©ã†ã‹
                    if not is_slot_value(shift_df.at[ridx, bg_hosp]):
                        continue
                    if isinstance(val, str) and val in doctor_names:
                        continue  # æ—¢ã«å‰²å½“æ¸ˆã¿

                    # ã“ã®æ—¥ã«zero_docãŒæ—¢ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                    already_assigned = False
                    for h in hospital_cols:
                        v = df.at[ridx, h]
                        if isinstance(v, str) and normalize_name(v) == zero_doc and h != hosp:
                            already_assigned = True
                            break

                    if already_assigned:
                        continue

                    # åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
                    if can_assign_doc_to_slot(zero_doc, date, bg_hosp):
                        # å¤–ç—…é™¢ã‹ã‚‰å‰Šé™¤ã€å¤§å­¦ç³»ã«è¿½åŠ 
                        df.at[ridx, hosp] = None  # å¤–ç—…é™¢ã‚’è§£é™¤
                        df.at[ridx, bg_hosp] = zero_doc  # å¤§å­¦ç³»ã«å‰²å½“
                        fixed_in_this_iteration += 1
                        total_fixed += 1
                        break  # æ¬¡ã®zero_docã¸

                if fixed_in_this_iteration > 0:
                    break  # æ¬¡ã®zero_docã¸

        # ä¿®æ­£ãŒé€²ã¾ãªãã¦ã‚‚max_attemptsã¾ã§è©¦è¡Œã‚’ç¶šã‘ã‚‹
        # if fixed_in_this_iteration == 0:
        #     break

    # æœ€çµ‚ç¢ºèª
    counts, bg_counts, *_ = recompute_stats(df)
    remaining_violations = sum(1 for doc in CODE_1_2_DOCTORS if bg_counts.get(doc, 0) == 0)

    if verbose:
        if remaining_violations == 0:
            print(f"   âœ… å…¨ã¦ã®å¯å¦ã‚³ãƒ¼ãƒ‰1.2åŒ»å¸«ã®å¤§å­¦ç³»0å›é•åã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")
        else:
            print(f"   âš ï¸ {remaining_violations}ä»¶ã®å¯å¦ã‚³ãƒ¼ãƒ‰1.2åŒ»å¸«ã®å¤§å­¦ç³»0å›é•åãŒæ®‹ã£ã¦ã„ã¾ã™ï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")

    return df, remaining_violations == 0, total_fixed

def fix_bg_ht_imbalance_violations(pattern_df, max_attempts=100, verbose=True):
    """
    å¤§å­¦ç³»ã¨å¤–ç—…é™¢ã®å·®ãŒ3ä»¥ä¸Šã®é•åã‚’ä¿®æ­£ã™ã‚‹

    Args:
        pattern_df: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DataFrame
        max_attempts: æœ€å¤§è©¦è¡Œå›æ•°
        verbose: ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã‹

    Returns:
        (ä¿®æ­£å¾Œã®DataFrame, æˆåŠŸãƒ•ãƒ©ã‚°, ä¿®æ­£æ•°)
    """
    df = pattern_df.copy()
    total_fixed = 0

    for attempt in range(max_attempts):
        # ç¾åœ¨ã®å‰²å½“å›æ•°ã‚’å†è¨ˆç®—
        counts, bg_counts, ht_counts, *_ = recompute_stats(df)

        # å¤§å­¦ç³»ã¨å¤–ç—…é™¢ã®å·®ãŒ3ä»¥ä¸Šã®åŒ»å¸«ã‚’ç‰¹å®š
        imbalance_docs = []
        for doc in active_doctors:
            bg = bg_counts.get(doc, 0)
            ht = ht_counts.get(doc, 0)
            diff = abs(bg - ht)
            if diff >= 3:
                imbalance_docs.append((doc, bg, ht, diff))

        if not imbalance_docs:
            if verbose and total_fixed > 0:
                print(f"   âœ… å¤§å­¦ç³»ã¨å¤–ç—…é™¢ã®å·®3ä»¥ä¸Šã®é•åã‚’{total_fixed}ä»¶ä¿®æ­£ã—ã¾ã—ãŸ")
            return df, True, total_fixed

        if attempt == 0 and verbose:
            imbalance_names = [f"{doc}(BG={bg}/HT={ht})" for doc, bg, ht, diff in imbalance_docs[:5]]
            print(f"   âš ï¸ å¤§å­¦ç³»ã¨å¤–ç—…é™¢ã®å·®3ä»¥ä¸Šã®é•åã‚’æ¤œå‡º â†’ è‡ªå‹•ä¿®æ­£ã‚’é–‹å§‹...")
            print(f"      å¯¾è±¡: {', '.join(imbalance_names)}")

        # ä¿®æ­£è©¦è¡Œ
        fixed_in_this_iteration = 0

        for doc, bg, ht, diff in imbalance_docs:
            if diff < 3:
                continue

            # BGãŒå¤šã„å ´åˆ: BGâ†’HTã«ç§»å‹•
            # HTãŒå¤šã„å ´åˆ: HTâ†’BGã«ç§»å‹•
            if bg > ht:
                # BGã®å‰²å½“ã‚’1ã¤HTã«å¤‰æ›´
                source_range = (B_COL_INDEX, B_K_END_INDEX)
                target_range = (L_COL_INDEX, L_Y_END_INDEX)
            else:
                # HTã®å‰²å½“ã‚’1ã¤BGã«å¤‰æ›´
                source_range = (L_COL_INDEX, L_Y_END_INDEX)
                target_range = (B_COL_INDEX, B_K_END_INDEX)

            # sourceç¯„å›²ã®å‰²å½“ã‚’æ¢ã™
            source_positions = []
            for ridx in df.index:
                date = df.at[ridx, date_col_shift]
                if pd.isna(date):
                    continue
                date = pd.to_datetime(date).normalize().tz_localize(None)

                for hosp in hospital_cols:
                    idx = shift_df.columns.get_loc(hosp)
                    if source_range[0] <= idx <= source_range[1]:
                        val = df.at[ridx, hosp]
                        if isinstance(val, str) and normalize_name(val) == doc:
                            source_positions.append((ridx, hosp, date))

            # 1ã¤ç§»å‹•ã‚’è©¦ã¿ã‚‹
            import random
            random.shuffle(source_positions)

            for ridx, hosp, date in source_positions[:1]:
                # targetç¯„å›²ã§ç©ºã„ã¦ã„ã‚‹æ ã‚’æ¢ã™
                for target_hosp in hospital_cols:
                    target_idx = shift_df.columns.get_loc(target_hosp)
                    if not (target_range[0] <= target_idx <= target_range[1]):
                        continue

                    val = df.at[ridx, target_hosp]
                    # ç©ºãæ ã‹ã©ã†ã‹
                    if not is_slot_value(shift_df.at[ridx, target_hosp]):
                        continue
                    if isinstance(val, str) and val in doctor_names:
                        continue  # æ—¢ã«å‰²å½“æ¸ˆã¿

                    # ã“ã®æ—¥ã«docãŒæ—¢ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                    already_assigned = False
                    for h in hospital_cols:
                        v = df.at[ridx, h]
                        if isinstance(v, str) and normalize_name(v) == doc and h != hosp:
                            already_assigned = True
                            break

                    if already_assigned:
                        continue

                    # åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
                    if can_assign_doc_to_slot(doc, date, target_hosp):
                        # sourceã‹ã‚‰å‰Šé™¤ã€targetã«è¿½åŠ 
                        df.at[ridx, hosp] = None
                        df.at[ridx, target_hosp] = doc
                        fixed_in_this_iteration += 1
                        total_fixed += 1
                        break  # æ¬¡ã®docã¸

                if fixed_in_this_iteration > 0:
                    break  # æ¬¡ã®docã¸

        # ä¿®æ­£ãŒé€²ã¾ãªãã¦ã‚‚max_attemptsã¾ã§è©¦è¡Œã‚’ç¶šã‘ã‚‹
        # if fixed_in_this_iteration == 0:
        #     break

    # æœ€çµ‚ç¢ºèª
    counts, bg_counts, ht_counts, *_ = recompute_stats(df)
    remaining_violations = sum(1 for doc in active_doctors if abs(bg_counts.get(doc, 0) - ht_counts.get(doc, 0)) >= 3)

    if verbose:
        if remaining_violations == 0:
            print(f"   âœ… å…¨ã¦ã®å¤§å­¦ç³»ã¨å¤–ç—…é™¢ã®å·®3ä»¥ä¸Šã®é•åã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")
        else:
            print(f"   âš ï¸ {remaining_violations}ä»¶ã®å¤§å­¦ç³»ã¨å¤–ç—…é™¢ã®å·®3ä»¥ä¸Šã®é•åãŒæ®‹ã£ã¦ã„ã¾ã™ï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")

    return df, remaining_violations == 0, total_fixed

def fix_gap_violations(pattern_df, max_attempts=200, verbose=True):
    """
    gapé•åï¼ˆ4æ—¥æœªæº€ã®é–“éš”ã§ã®å‰²å½“ï¼‰ã‚’ä¿®æ­£ã™ã‚‹

    Args:
        pattern_df: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DataFrame
        max_attempts: æœ€å¤§è©¦è¡Œå›æ•°
        verbose: ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã‹

    Returns:
        (ä¿®æ­£å¾Œã®DataFrame, æˆåŠŸãƒ•ãƒ©ã‚°, ä¿®æ­£æ•°)
    """
    df = pattern_df.copy()
    total_fixed = 0
    consecutive_failures = 0

    for attempt in range(max_attempts):
        # ç¾åœ¨ã®å‰²å½“çŠ¶æ…‹ã‚’å†è¨ˆç®—
        counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)

        # gapé•åã‚’æ¤œå‡º
        gap_violation_list = []
        for doc, date_hosp_list in doc_assignments.items():
            dates = sorted([d for d, h in date_hosp_list])
            for i in range(1, len(dates)):
                gap = (dates[i] - dates[i-1]).days
                if gap < 4:
                    gap_violation_list.append((doc, dates[i-1], dates[i], gap))

        if not gap_violation_list:
            if verbose and total_fixed > 0:
                print(f"   âœ… gapé•åï¼ˆ4æ—¥æœªæº€ã®é–“éš”ï¼‰ã‚’{total_fixed}ä»¶ä¿®æ­£ã—ã¾ã—ãŸ")
            return df, True, total_fixed

        if attempt == 0 and verbose:
            violation_names = [f"{doc}({d1.strftime('%m/%d')}-{d2.strftime('%m/%d')}={gap}æ—¥)"
                             for doc, d1, d2, gap in gap_violation_list[:5]]
            print(f"   âš ï¸ gapé•åã‚’{len(gap_violation_list)}ä»¶æ¤œå‡º â†’ è‡ªå‹•ä¿®æ­£ã‚’é–‹å§‹...")
            print(f"      ä¾‹: {', '.join(violation_names)}")

        # ä¿®æ­£è©¦è¡Œï¼ˆ1ã‚¤ãƒ†ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³ã§è¤‡æ•°ã®é•åã‚’ä¿®æ­£ï¼‰
        fixed_in_this_iteration = 0

        for doc, date1, date2, gap in gap_violation_list:
            if gap >= 4:
                continue

            # date2ã®å‰²å½“ã‚’æ¢ã™
            positions_at_date2 = []
            for ridx in df.index:
                date = df.at[ridx, date_col_shift]
                if pd.isna(date):
                    continue
                date = pd.to_datetime(date).normalize().tz_localize(None)
                if date != date2:
                    continue

                for hosp in hospital_cols:
                    val = df.at[ridx, hosp]
                    if isinstance(val, str) and normalize_name(val) == doc:
                        positions_at_date2.append((ridx, hosp, date))

            # å„positionã«å¯¾ã—ã¦ä¿®æ­£ã‚’è©¦ã¿ã‚‹
            for ridx_src, hosp_src, date_src in positions_at_date2:
                moved = False

                # ç§»å‹•å…ˆå€™è£œã‚’æ¢ã™
                for ridx_tgt in df.index:
                    date_tgt = df.at[ridx_tgt, date_col_shift]
                    if pd.isna(date_tgt):
                        continue
                    date_tgt = pd.to_datetime(date_tgt).normalize().tz_localize(None)

                    # date1ã¨date_tgtã®é–“éš”ã‚’ãƒã‚§ãƒƒã‚¯
                    gap_from_date1 = abs((date_tgt - date1).days)
                    if gap_from_date1 < 4:
                        continue

                    # docã®ä»–ã®å‰²å½“ã¨date_tgtã®é–“éš”ã‚’ãƒã‚§ãƒƒã‚¯
                    doc_dates = sorted([d for d, h in doc_assignments[doc]])
                    doc_dates_without_date2 = [d for d in doc_dates if d != date2]

                    valid_gap = True
                    for existing_date in doc_dates_without_date2:
                        if abs((date_tgt - existing_date).days) < 4:
                            valid_gap = False
                            break

                    if not valid_gap:
                        continue

                    # ãã®æ—¥ã«docãŒæ—¢ã«å‰²å½“ã‚‰ã‚Œã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
                    already_assigned = False
                    for hosp_check in hospital_cols:
                        val = df.at[ridx_tgt, hosp_check]
                        if isinstance(val, str) and normalize_name(val) == doc:
                            already_assigned = True
                            break

                    if already_assigned:
                        continue

                    # å…¨ã¦ã®ç—…é™¢ã§ç©ºãæ ã‚’æ¢ã™
                    hospitals_to_try = [hosp_src] + [h for h in hospital_cols if h != hosp_src]
                    for hosp_tgt in hospitals_to_try:
                        if pd.isna(df.at[ridx_tgt, hosp_tgt]):
                            # ãƒãƒ¼ãƒ‰åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
                            if not can_assign_doc_to_slot(doc, date_tgt, hosp_tgt):
                                continue

                            # ç§»å‹•å®Ÿè¡Œ
                            df.at[ridx_src, hosp_src] = None
                            df.at[ridx_tgt, hosp_tgt] = doc
                            fixed_in_this_iteration += 1
                            total_fixed += 1
                            moved = True
                            break

                    if moved:
                        break

                # ç§»å‹•å…ˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å‰Šé™¤ï¼ˆç©æ¥µçš„ã«å®Ÿè¡Œï¼‰
                if not moved and attempt >= 5:  # 5å›ç›®ä»¥é™ã¯å‰Šé™¤ã‚‚æ¤œè¨
                    df.at[ridx_src, hosp_src] = None
                    fixed_in_this_iteration += 1
                    total_fixed += 1
                    if verbose and attempt < 10:
                        print(f"      ç§»å‹•å…ˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€{doc}ã®{date_src.strftime('%m/%d')}ã®å‰²å½“ã‚’å‰Šé™¤ã—ã¾ã™")
                    break  # ã“ã®é•åã®ä»–ã®positionã¯è©¦ã•ãªã„

            # ã“ã®é•åã‚’ä¿®æ­£ã—ãŸã‚‰ã€doc_assignmentsã‚’æ›´æ–°
            if fixed_in_this_iteration > 0:
                counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)

        # é€²æ—ãƒã‚§ãƒƒã‚¯
        if fixed_in_this_iteration == 0:
            consecutive_failures += 1
        else:
            consecutive_failures = 0

        # é€£ç¶šã§20å›ä¿®æ­£ã§ããªã‘ã‚Œã°è«¦ã‚ã‚‹
        if consecutive_failures >= 20:
            break

    # æœ€çµ‚ç¢ºèª
    counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)
    remaining_violations = 0
    for doc, date_hosp_list in doc_assignments.items():
        dates = sorted([d for d, h in date_hosp_list])
        for i in range(1, len(dates)):
            if (dates[i] - dates[i-1]).days < 4:
                remaining_violations += 1

    if verbose:
        if remaining_violations == 0:
            print(f"   âœ… å…¨ã¦ã®gapé•åã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")
        else:
            print(f"   âš ï¸ {remaining_violations}ä»¶ã®gapé•åãŒæ®‹ã£ã¦ã„ã¾ã™ï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")

    return df, remaining_violations == 0, total_fixed

def fix_external_hospital_dup_violations(pattern_df, max_attempts=150, verbose=True):
    """
    å¤–ç—…é™¢ï¼ˆLï½Yåˆ—ï¼‰ã®é‡è¤‡ã‚’ä¿®æ­£ã™ã‚‹ï¼ˆå¤§å­¦ç—…é™¢ã®é‡è¤‡ã¯è¨±å®¹ï¼‰

    Args:
        pattern_df: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DataFrame
        max_attempts: æœ€å¤§è©¦è¡Œå›æ•°
        verbose: ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã‹

    Returns:
        (ä¿®æ­£å¾Œã®DataFrame, æˆåŠŸãƒ•ãƒ©ã‚°, ä¿®æ­£æ•°)
    """
    df = pattern_df.copy()
    total_fixed = 0
    consecutive_failures = 0

    for attempt in range(max_attempts):
        # ç¾åœ¨ã®å‰²å½“çŠ¶æ…‹ã‚’å†è¨ˆç®—
        counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)

        # å¤–ç—…é™¢é‡è¤‡ã‚’æ¤œå‡º
        external_dup_list = []
        for doc, hosp_dict in assigned_hosp_count.items():
            for hosp, count in hosp_dict.items():
                if count > 1:
                    # å¤–ç—…é™¢ã‹ã©ã†ã‹ã‚’åˆ¤å®š
                    hidx = shift_df.columns.get_loc(hosp)
                    if L_COL_INDEX <= hidx <= L_Y_END_INDEX:
                        external_dup_list.append((doc, hosp, count))

        if not external_dup_list:
            if verbose and total_fixed > 0:
                print(f"   âœ… å¤–ç—…é™¢é‡è¤‡ã‚’{total_fixed}ä»¶ä¿®æ­£ã—ã¾ã—ãŸ")
            return df, True, total_fixed

        if attempt == 0 and verbose:
            dup_names = [f"{doc}({hosp}={count}å›)" for doc, hosp, count in external_dup_list[:5]]
            print(f"   âš ï¸ å¤–ç—…é™¢é‡è¤‡ã‚’{len(external_dup_list)}ä»¶æ¤œå‡º â†’ è‡ªå‹•ä¿®æ­£ã‚’é–‹å§‹...")
            print(f"      ä¾‹: {', '.join(dup_names)}")

        # ä¿®æ­£è©¦è¡Œ
        fixed_in_this_iteration = 0

        for doc, dup_hosp, count in external_dup_list:
            if count <= 1:
                continue

            # ã“ã®åŒ»å¸«ã®ã“ã®ç—…é™¢ã¸ã®å‰²å½“ã‚’æ¢ã™
            dup_positions = []
            for ridx in df.index:
                date = df.at[ridx, date_col_shift]
                if pd.isna(date):
                    continue
                date = pd.to_datetime(date).normalize().tz_localize(None)

                val = df.at[ridx, dup_hosp]
                if isinstance(val, str) and normalize_name(val) == doc:
                    dup_positions.append((ridx, dup_hosp, date))

            # é‡è¤‡ã®ã†ã¡1ã¤ã‚’æ®‹ã—ã¦ã€æ®‹ã‚Šã‚’åˆ¥ã®ç—…é™¢ã«ç§»å‹•ã¾ãŸã¯å‰Šé™¤
            import random
            random.shuffle(dup_positions)

            for ridx, hosp, date in dup_positions[1:]:  # æœ€åˆã®1ã¤ã¯æ®‹ã™
                moved = False

                # åŒã˜æ—¥ã®ä»–ã®å¤–ç—…é™¢ï¼ˆLï½Yåˆ—ï¼‰ã®ç©ºãæ ã‚’æ¢ã™
                for other_hosp in hospital_cols:
                    other_hidx = shift_df.columns.get_loc(other_hosp)
                    # å¤–ç—…é™¢ã‹ã¤é‡è¤‡ç—…é™¢ã§ãªã„
                    if not (L_COL_INDEX <= other_hidx <= L_Y_END_INDEX):
                        continue
                    if other_hosp == dup_hosp:
                        continue

                    # ã“ã®ç—…é™¢ã«ã“ã®åŒ»å¸«ãŒæ—¢ã«å‰²å½“ã‚‰ã‚Œã¦ã„ãªã„ã‹
                    if assigned_hosp_count[doc].get(other_hosp, 0) >= 1:
                        continue

                    # ç©ºãæ ãŒã‚ã‚‹ã‹
                    if pd.isna(df.at[ridx, other_hosp]):
                        # ãƒãƒ¼ãƒ‰åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
                        if not can_assign_doc_to_slot(doc, date, other_hosp):
                            continue

                        # ç§»å‹•å®Ÿè¡Œ
                        df.at[ridx, dup_hosp] = None
                        df.at[ridx, other_hosp] = doc
                        fixed_in_this_iteration += 1
                        total_fixed += 1
                        moved = True
                        break

                # ç§»å‹•å…ˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å‰Šé™¤
                if not moved and attempt >= 5:
                    df.at[ridx, dup_hosp] = None
                    fixed_in_this_iteration += 1
                    total_fixed += 1
                    if verbose and attempt < 10:
                        print(f"      ç§»å‹•å…ˆãŒè¦‹ã¤ã‹ã‚‰ãªã„ãŸã‚ã€{doc}ã®{date.strftime('%m/%d')}ã®{dup_hosp}å‰²å½“ã‚’å‰Šé™¤ã—ã¾ã™")
                    break  # ã“ã®é‡è¤‡ã®ä»–ã®positionã¯æ¬¡å›

            if fixed_in_this_iteration > 0:
                counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)

        # é€²æ—ãƒã‚§ãƒƒã‚¯
        if fixed_in_this_iteration == 0:
            consecutive_failures += 1
        else:
            consecutive_failures = 0

        # é€£ç¶šã§20å›ä¿®æ­£ã§ããªã‘ã‚Œã°è«¦ã‚ã‚‹
        if consecutive_failures >= 20:
            break

    # æœ€çµ‚ç¢ºèª
    counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)
    remaining_violations = 0
    for doc, hosp_dict in assigned_hosp_count.items():
        for hosp, count in hosp_dict.items():
            if count > 1:
                hidx = shift_df.columns.get_loc(hosp)
                if L_COL_INDEX <= hidx <= L_Y_END_INDEX:
                    remaining_violations += (count - 1)

    if verbose:
        if remaining_violations == 0:
            print(f"   âœ… å…¨ã¦ã®å¤–ç—…é™¢é‡è¤‡ã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")
        else:
            print(f"   âš ï¸ {remaining_violations}ä»¶ã®å¤–ç—…é™¢é‡è¤‡ãŒæ®‹ã£ã¦ã„ã¾ã™ï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")

    return df, remaining_violations == 0, total_fixed

def fix_university_over_2_violations(pattern_df, max_attempts=150, verbose=True):
    """
    å¤§å­¦ç—…é™¢ï¼ˆBï½Kåˆ—ï¼‰ãŒ3å›ä»¥ä¸Šã®åŒ»å¸«ã®é•åã‚’ä¿®æ­£ã™ã‚‹

    Args:
        pattern_df: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DataFrame
        max_attempts: æœ€å¤§è©¦è¡Œå›æ•°
        verbose: ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã‹

    Returns:
        (ä¿®æ­£å¾Œã®DataFrame, æˆåŠŸãƒ•ãƒ©ã‚°, ä¿®æ­£æ•°)
    """
    df = pattern_df.copy()
    total_fixed = 0
    consecutive_failures = 0

    for attempt in range(max_attempts):
        # ç¾åœ¨ã®å‰²å½“çŠ¶æ…‹ã‚’å†è¨ˆç®—
        counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)

        # å¤§å­¦3å›ä»¥ä¸Šã®åŒ»å¸«ã‚’æ¤œå‡º
        over_2_list = []
        for doc in active_doctors:
            bg_count = bg_counts.get(doc, 0)
            if bg_count >= 3:
                over_2_list.append((doc, bg_count))

        if not over_2_list:
            if verbose and total_fixed > 0:
                print(f"   âœ… å¤§å­¦3å›ä»¥ä¸Šé•åã‚’{total_fixed}ä»¶ä¿®æ­£ã—ã¾ã—ãŸ")
            return df, True, total_fixed

        if attempt == 0 and verbose:
            over_names = [f"{doc}({bg_count}å›)" for doc, bg_count in over_2_list[:5]]
            print(f"   âš ï¸ å¤§å­¦3å›ä»¥ä¸Šé•åã‚’{len(over_2_list)}ä»¶æ¤œå‡º â†’ è‡ªå‹•ä¿®æ­£ã‚’é–‹å§‹...")
            print(f"      å¯¾è±¡: {', '.join(over_names)}")

        # ä¿®æ­£è©¦è¡Œ
        fixed_in_this_iteration = 0

        for doc, bg_count in over_2_list:
            if bg_count < 3:
                continue

            # ã“ã®åŒ»å¸«ã®å¤§å­¦ç—…é™¢ã¸ã®å‰²å½“ã‚’æ¢ã™
            bg_positions = []
            for ridx in df.index:
                date = df.at[ridx, date_col_shift]
                if pd.isna(date):
                    continue
                date = pd.to_datetime(date).normalize().tz_localize(None)

                for hosp in hospital_cols:
                    hidx = shift_df.columns.get_loc(hosp)
                    # å¤§å­¦ç—…é™¢ï¼ˆBï½Kåˆ—ï¼‰ã‹
                    if not (B_COL_INDEX <= hidx <= B_K_END_INDEX):
                        continue

                    val = df.at[ridx, hosp]
                    if isinstance(val, str) and normalize_name(val) == doc:
                        bg_positions.append((ridx, hosp, date))

            # 3å›ä»¥ä¸Šã®ã†ã¡ã€å‰Šæ¸›ã™ã‚‹ï¼ˆ2å›ã¾ã§æ¸›ã‚‰ã™ï¼‰
            excess = bg_count - 2
            import random
            random.shuffle(bg_positions)

            for ridx, hosp, date in bg_positions[:excess]:
                moved = False

                # åŒã˜æ—¥ã®å¤–ç—…é™¢ï¼ˆLï½Yåˆ—ï¼‰ã®ç©ºãæ ã«ç§»å‹•ã‚’è©¦ã¿ã‚‹
                for other_hosp in hospital_cols:
                    other_hidx = shift_df.columns.get_loc(other_hosp)
                    # å¤–ç—…é™¢ã‹
                    if not (L_COL_INDEX <= other_hidx <= L_Y_END_INDEX):
                        continue

                    # ã“ã®ç—…é™¢ã«ã“ã®åŒ»å¸«ãŒæ—¢ã«å‰²å½“ã‚‰ã‚Œã¦ã„ãªã„ã‹
                    if assigned_hosp_count[doc].get(other_hosp, 0) >= 1:
                        continue

                    # ç©ºãæ ãŒã‚ã‚‹ã‹
                    if pd.isna(df.at[ridx, other_hosp]):
                        # ãƒãƒ¼ãƒ‰åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
                        if not can_assign_doc_to_slot(doc, date, other_hosp):
                            continue

                        # ç§»å‹•å®Ÿè¡Œ
                        df.at[ridx, hosp] = None
                        df.at[ridx, other_hosp] = doc
                        fixed_in_this_iteration += 1
                        total_fixed += 1
                        moved = True
                        break

                # ç§»å‹•å…ˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å‰Šé™¤
                if not moved and attempt >= 5:
                    df.at[ridx, hosp] = None
                    fixed_in_this_iteration += 1
                    total_fixed += 1
                    if verbose and attempt < 10:
                        print(f"      {doc}ã®{date.strftime('%m/%d')}ã®å¤§å­¦ç—…é™¢å‰²å½“ã‚’å‰Šé™¤ã—ã¾ã™ï¼ˆ3å›ä»¥ä¸Šâ†’2å›ï¼‰")

            if fixed_in_this_iteration > 0:
                counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)

        # é€²æ—ãƒã‚§ãƒƒã‚¯
        if fixed_in_this_iteration == 0:
            consecutive_failures += 1
        else:
            consecutive_failures = 0

        # é€£ç¶šã§20å›ä¿®æ­£ã§ããªã‘ã‚Œã°è«¦ã‚ã‚‹
        if consecutive_failures >= 20:
            break

    # æœ€çµ‚ç¢ºèª
    counts, bg_counts, *_ = recompute_stats(df)
    remaining_violations = sum(1 for doc in active_doctors if bg_counts.get(doc, 0) >= 3)

    if verbose:
        if remaining_violations == 0:
            print(f"   âœ… å…¨ã¦ã®å¤§å­¦3å›ä»¥ä¸Šé•åã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")
        else:
            print(f"   âš ï¸ {remaining_violations}ä»¶ã®å¤§å­¦3å›ä»¥ä¸Šé•åãŒæ®‹ã£ã¦ã„ã¾ã™ï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")

    return df, remaining_violations == 0, total_fixed

def fix_university_weekday_balance_violations(pattern_df, max_attempts=150, verbose=True):
    """
    å¤§å­¦ç—…é™¢ã®å¹³æ—¥åã‚Šï¼ˆå¹³æ—¥2å›ä»¥ä¸Šï¼‰ã®é•åã‚’ä¿®æ­£ã™ã‚‹

    Args:
        pattern_df: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DataFrame
        max_attempts: æœ€å¤§è©¦è¡Œå›æ•°
        verbose: ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã‹

    Returns:
        (ä¿®æ­£å¾Œã®DataFrame, æˆåŠŸãƒ•ãƒ©ã‚°, ä¿®æ­£æ•°)
    """
    df = pattern_df.copy()
    total_fixed = 0
    consecutive_failures = 0

    for attempt in range(max_attempts):
        # ç¾åœ¨ã®å‰²å½“çŠ¶æ…‹ã‚’å†è¨ˆç®—
        counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)

        # å¤§å­¦ã®å¹³æ—¥2å›ä»¥ä¸Šã®åŒ»å¸«ã‚’æ¤œå‡º
        weekday_over_list = []
        for doc in active_doctors:
            weekday_count = bg_cat[doc].get("å¹³æ—¥", 0)
            if weekday_count >= 2:
                weekday_over_list.append((doc, weekday_count, bg_counts.get(doc, 0)))

        if not weekday_over_list:
            if verbose and total_fixed > 0:
                print(f"   âœ… å¤§å­¦å¹³æ—¥åã‚Šé•åã‚’{total_fixed}ä»¶ä¿®æ­£ã—ã¾ã—ãŸ")
            return df, True, total_fixed

        if attempt == 0 and verbose:
            over_names = [f"{doc}(å¹³æ—¥{wd}å›/å¤§å­¦{total}å›)" for doc, wd, total in weekday_over_list[:5]]
            print(f"   âš ï¸ å¤§å­¦å¹³æ—¥åã‚Šé•åã‚’{len(weekday_over_list)}ä»¶æ¤œå‡º â†’ è‡ªå‹•ä¿®æ­£ã‚’é–‹å§‹...")
            print(f"      å¯¾è±¡: {', '.join(over_names)}")

        # ä¿®æ­£è©¦è¡Œ
        fixed_in_this_iteration = 0

        for doc, weekday_count, bg_total in weekday_over_list:
            if weekday_count < 2:
                continue

            # ã“ã®åŒ»å¸«ã®å¤§å­¦ç—…é™¢å¹³æ—¥ã®å‰²å½“ã‚’æ¢ã™
            bg_weekday_positions = []
            for ridx in df.index:
                date = df.at[ridx, date_col_shift]
                if pd.isna(date):
                    continue
                date = pd.to_datetime(date).normalize().tz_localize(None)

                for hosp in hospital_cols:
                    hidx = shift_df.columns.get_loc(hosp)
                    # å¤§å­¦ç—…é™¢ï¼ˆBï½Kåˆ—ï¼‰ã‹
                    if not (B_COL_INDEX <= hidx <= B_K_END_INDEX):
                        continue

                    val = df.at[ridx, hosp]
                    if isinstance(val, str) and normalize_name(val) == doc:
                        # å¹³æ—¥ã‹
                        category = classify_bg_category(date, hosp)
                        if category == "å¹³æ—¥":
                            bg_weekday_positions.append((ridx, hosp, date))

            # å¹³æ—¥ã®ã†ã¡1ã¤ã‚’å¤–ç—…é™¢ã«ç§»å‹•ã¾ãŸã¯å‰Šé™¤
            import random
            random.shuffle(bg_weekday_positions)

            for ridx, hosp, date in bg_weekday_positions[:1]:  # 1ã¤ã ã‘è©¦è¡Œ
                moved = False

                # åŒã˜æ—¥ã®å¤–ç—…é™¢ï¼ˆLï½Yåˆ—ï¼‰ã®ç©ºãæ ã«ç§»å‹•ã‚’è©¦ã¿ã‚‹
                for other_hosp in hospital_cols:
                    other_hidx = shift_df.columns.get_loc(other_hosp)
                    # å¤–ç—…é™¢ã‹
                    if not (L_COL_INDEX <= other_hidx <= L_Y_END_INDEX):
                        continue

                    # ã“ã®ç—…é™¢ã«ã“ã®åŒ»å¸«ãŒæ—¢ã«å‰²å½“ã‚‰ã‚Œã¦ã„ãªã„ã‹
                    if assigned_hosp_count[doc].get(other_hosp, 0) >= 1:
                        continue

                    # ç©ºãæ ãŒã‚ã‚‹ã‹
                    if pd.isna(df.at[ridx, other_hosp]):
                        # ãƒãƒ¼ãƒ‰åˆ¶ç´„ãƒã‚§ãƒƒã‚¯
                        if not can_assign_doc_to_slot(doc, date, other_hosp):
                            continue

                        # ç§»å‹•å®Ÿè¡Œ
                        df.at[ridx, hosp] = None
                        df.at[ridx, other_hosp] = doc
                        fixed_in_this_iteration += 1
                        total_fixed += 1
                        moved = True
                        break

                # ç§»å‹•å…ˆãŒè¦‹ã¤ã‹ã‚‰ãªã„å ´åˆã¯å‰Šé™¤
                if not moved and attempt >= 5:
                    df.at[ridx, hosp] = None
                    fixed_in_this_iteration += 1
                    total_fixed += 1
                    if verbose and attempt < 10:
                        print(f"      {doc}ã®{date.strftime('%m/%d')}ã®å¤§å­¦å¹³æ—¥å‰²å½“ã‚’å‰Šé™¤ã—ã¾ã™")
                    break

            if fixed_in_this_iteration > 0:
                counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)

        # é€²æ—ãƒã‚§ãƒƒã‚¯
        if fixed_in_this_iteration == 0:
            consecutive_failures += 1
        else:
            consecutive_failures = 0

        # é€£ç¶šã§20å›ä¿®æ­£ã§ããªã‘ã‚Œã°è«¦ã‚ã‚‹
        if consecutive_failures >= 20:
            break

    # æœ€çµ‚ç¢ºèª
    counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, *_ = recompute_stats(df)
    remaining_violations = sum(1 for doc in active_doctors if bg_cat[doc].get("å¹³æ—¥", 0) >= 2)

    if verbose:
        if remaining_violations == 0:
            print(f"   âœ… å…¨ã¦ã®å¤§å­¦å¹³æ—¥åã‚Šé•åã‚’ä¿®æ­£ã—ã¾ã—ãŸï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")
        else:
            print(f"   âš ï¸ {remaining_violations}ä»¶ã®å¤§å­¦å¹³æ—¥åã‚Šé•åãŒæ®‹ã£ã¦ã„ã¾ã™ï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")

    return df, remaining_violations == 0, total_fixed

def fix_fairness_imbalance(pattern_df, max_attempts=200, verbose=True):
    """
    activeåŒ»å¸«é–“ã®å‰²å½“å›æ•°ã®å…¬å¹³æ€§ã‚’å¼·åŒ–ã™ã‚‹ï¼ˆæœ€å¤§ã¨æœ€å°ã®å·®ã‚’ç¸®ã‚ã‚‹ï¼‰

    Args:
        pattern_df: ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«DataFrame
        max_attempts: æœ€å¤§è©¦è¡Œå›æ•°
        verbose: ãƒ­ã‚°å‡ºåŠ›ã™ã‚‹ã‹

    Returns:
        (ä¿®æ­£å¾Œã®DataFrame, æˆåŠŸãƒ•ãƒ©ã‚°, ä¿®æ­£æ•°)
    """
    df = pattern_df.copy()
    total_fixed = 0
    consecutive_failures = 0

    for attempt in range(max_attempts):
        counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(df)

        # activeåŒ»å¸«ã®å‰²å½“å›æ•°ã‚’ç¢ºèª
        active_counts = [(doc, counts.get(doc, 0)) for doc in active_doctors]
        if not active_counts:
            return df, True, total_fixed

        # æœ€å¤§ã¨æœ€å°ã‚’å–å¾—
        max_count = max(c for _, c in active_counts)
        min_count = min(c for _, c in active_counts)
        diff = max_count - min_count

        # å·®ãŒ1ä»¥ä¸‹ãªã‚‰å…¬å¹³æ€§é”æˆ
        if diff <= 1:
            if verbose and total_fixed > 0:
                print(f"   âœ… å…¬å¹³æ€§é•åã‚’{total_fixed}ä»¶ä¿®æ­£ã—ã¾ã—ãŸï¼ˆmax={max_count}, min={min_count}, diff={diff}ï¼‰")
            return df, True, total_fixed

        if attempt == 0 and verbose:
            max_docs = [doc for doc, c in active_counts if c == max_count]
            min_docs = [doc for doc, c in active_counts if c == min_count]
            print(f"   âš ï¸ å…¬å¹³æ€§é•åã‚’æ¤œå‡ºï¼ˆmax={max_count}, min={min_count}, diff={diff}ï¼‰ â†’ è‡ªå‹•ä¿®æ­£ã‚’é–‹å§‹...")
            print(f"      æœ€å¤š: {', '.join(max_docs[:3])}... ({len(max_docs)}äºº)")
            print(f"      æœ€å°‘: {', '.join(min_docs[:3])}... ({len(min_docs)}äºº)")

        fixed_in_this_iteration = 0

        # æœ€å¤§å›æ•°ã®åŒ»å¸«ã‹ã‚‰æœ€å°å›æ•°ã®åŒ»å¸«ã«ã‚·ãƒ•ãƒˆã‚’ç§»å‹•
        max_docs = [doc for doc, c in active_counts if c == max_count]
        min_docs = [doc for doc, c in active_counts if c == min_count]

        import random
        random.shuffle(max_docs)
        random.shuffle(min_docs)

        # æœ€å¤§å›æ•°ã®åŒ»å¸«ã®ã‚·ãƒ•ãƒˆã‚’æ¢ã™
        for max_doc in max_docs[:3]:  # æœ€å¤§3äººã¾ã§è©¦è¡Œ
            # max_docã®å‰²å½“ä½ç½®ã‚’å–å¾—
            max_doc_positions = []
            for ridx in df.index:
                date = df.at[ridx, date_col_shift]
                if pd.isna(date):
                    continue
                date = pd.to_datetime(date).normalize().tz_localize(None)

                for hosp in hospital_cols:
                    val = df.at[ridx, hosp]
                    if isinstance(val, str) and normalize_name(val) == max_doc:
                        max_doc_positions.append((ridx, hosp, date))

            random.shuffle(max_doc_positions)

            # å„ä½ç½®ã«ã¤ã„ã¦ã€æœ€å°å›æ•°ã®åŒ»å¸«ã¨å…¥ã‚Œæ›¿ãˆå¯èƒ½ã‹è©¦ã™
            for ridx, hosp, date in max_doc_positions[:5]:  # æœ€å¤§5å€‹ã¾ã§è©¦è¡Œ
                # ã“ã®æ—¥ã«æ—¢ã«å‰²ã‚Šå½“ã¦ã‚‰ã‚Œã¦ã„ã‚‹åŒ»å¸«ã‚’é™¤å¤–
                already_assigned_on_date = set()
                for h in hospital_cols:
                    v = df.at[ridx, h]
                    if isinstance(v, str):
                        already_assigned_on_date.add(normalize_name(v))

                # æœ€å°å›æ•°ã®åŒ»å¸«ã®ä¸­ã‹ã‚‰ä»£æ›¿ã‚’æ¢ã™
                for min_doc in min_docs:
                    if min_doc in already_assigned_on_date:
                        continue
                    if not can_assign_doc_to_slot(min_doc, date, hosp):
                        continue

                    # gapåˆ¶ç´„ãƒã‚§ãƒƒã‚¯ï¼ˆç§»å‹•å¾Œã«gapé•åãŒç™ºç”Ÿã—ãªã„ã‹ï¼‰
                    # min_docã«å‰²ã‚Šå½“ã¦ãŸå ´åˆã®gapé•åãƒã‚§ãƒƒã‚¯
                    min_doc_dates = sorted([d for r, h, d in doc_assignments.get(min_doc, []) if (r, h) != (ridx, hosp)])
                    new_dates = sorted(min_doc_dates + [date])

                    gap_ok = True
                    for j in range(len(new_dates) - 1):
                        gap = (new_dates[j + 1] - new_dates[j]).days
                        if gap < 4:
                            gap_ok = False
                            break

                    if not gap_ok:
                        continue

                    # max_docã‹ã‚‰å‰Šé™¤ã—ãŸå ´åˆã®gapé•åãƒã‚§ãƒƒã‚¯
                    max_doc_dates = sorted([d for r, h, d in doc_assignments.get(max_doc, []) if (r, h) != (ridx, hosp)])
                    if len(max_doc_dates) >= 2:
                        for j in range(len(max_doc_dates) - 1):
                            gap = (max_doc_dates[j + 1] - max_doc_dates[j]).days
                            # å‰Šé™¤ã«ã‚ˆã£ã¦gapé•åãŒç™ºç”Ÿã™ã‚‹ã“ã¨ã¯ãªã„ï¼ˆå‰Šé™¤ã¯é–“éš”ã‚’åºƒã’ã‚‹ã ã‘ï¼‰

                    # å…¥ã‚Œæ›¿ãˆ
                    df.at[ridx, hosp] = min_doc
                    fixed_in_this_iteration += 1
                    total_fixed += 1

                    if verbose and attempt < 5:
                        print(f"      {date.strftime('%m/%d')} {hosp}: {max_doc}({max_count}å›) â†’ {min_doc}({min_count}å›)")

                    # doc_assignmentsã‚’æ›´æ–°ï¼ˆæ¬¡ã®åå¾©ã®ãŸã‚ï¼‰
                    if max_doc in doc_assignments:
                        doc_assignments[max_doc] = [(r, h, d) for r, h, d in doc_assignments[max_doc] if (r, h) != (ridx, hosp)]
                    if min_doc not in doc_assignments:
                        doc_assignments[min_doc] = []
                    doc_assignments[min_doc].append((ridx, hosp, date))

                    break  # min_docs loop

                if fixed_in_this_iteration > 0:
                    break  # max_doc_positions loop

            if fixed_in_this_iteration > 0:
                break  # max_docs loop

        # é€²æ—ãƒã‚§ãƒƒã‚¯
        if fixed_in_this_iteration == 0:
            consecutive_failures += 1
        else:
            consecutive_failures = 0

        # é€£ç¶šã§20å›ä¿®æ­£ã§ããªã‘ã‚Œã°è«¦ã‚ã‚‹
        if consecutive_failures >= 20:
            break

    # æœ€çµ‚ç¢ºèª
    counts, *_ = recompute_stats(df)
    active_counts = [(doc, counts.get(doc, 0)) for doc in active_doctors]
    max_count = max(c for _, c in active_counts)
    min_count = min(c for _, c in active_counts)
    diff = max_count - min_count

    if verbose:
        if diff <= 1:
            print(f"   âœ… å…¬å¹³æ€§ã‚’é”æˆã—ã¾ã—ãŸï¼ˆmax={max_count}, min={min_count}, diff={diff}ï¼‰ï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")
        else:
            print(f"   âš ï¸ å…¬å¹³æ€§é•åãŒæ®‹ã£ã¦ã„ã¾ã™ï¼ˆmax={max_count}, min={min_count}, diff={diff}ï¼‰ï¼ˆä¿®æ­£æ•°: {total_fixed}ï¼‰")

    return df, diff <= 1, total_fixed

def build_diagnostics(pattern_df):
    counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, assigned_hosp_count, doc_assignments, unassigned = recompute_stats(pattern_df)
    score, raw, metrics = evaluate_schedule_with_raw(
        pattern_df,
        counts,
        bg_counts,
        ht_counts,
        wd_counts,
        we_counts,
        bk_counts,
        ly_counts,
    )

    df_doctors = build_doctor_diag(counts, bg_counts, ht_counts, wd_counts, we_counts, doc_assignments, assigned_hosp_count)
    df_gap = build_gap_details(doc_assignments)
    df_same = build_same_day_duplicates(doc_assignments)
    df_hdup = build_hosp_dup_details(assigned_hosp_count)
    df_unass = build_unassigned_details(unassigned)
    df_metrics = build_metrics_df(score, raw, metrics)
    df_hard_violations = build_hard_constraint_violations(pattern_df)

    return df_doctors, df_gap, df_same, df_hdup, df_unass, df_metrics, df_hard_violations

# =========================
# ãƒ‘ã‚¿ãƒ¼ãƒ³æ¢ç´¢ï¼ˆgreedy â†’ topå€™è£œã«å±€æ‰€æ¢ç´¢ â†’ top3ï¼‰
# =========================
print("\nğŸš€ ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ç”Ÿæˆã‚’é–‹å§‹ã—ã¾ã™...")
print(f"   ãƒ‘ã‚¿ãƒ¼ãƒ³æ•°: {NUM_PATTERNS}")
print(f"   å±€æ‰€æ¢ç´¢: {'æœ‰åŠ¹' if LOCAL_SEARCH_ENABLED else 'ç„¡åŠ¹'}")
print(f"   â€»å‡¦ç†æ™‚é–“: ç´„5-10åˆ†ï¼ˆãƒ‘ã‚¿ãƒ¼ãƒ³æ•°ã«ä¾å­˜ï¼‰\n")

score_rows = []
candidates = []  # TOP_KEEPã ã‘ä¿æŒ

for i in range(1, NUM_PATTERNS + 1):
    if i % 100 == 0 or i == 1:
        print(f"   é€²æ—: {i}/{NUM_PATTERNS} ãƒ‘ã‚¿ãƒ¼ãƒ³ç”Ÿæˆä¸­...")

    (
        pattern_df,
        counts,
        bg_counts,
        ht_counts,
        wd_counts,
        we_counts,
        bk_counts,
        ly_counts,
        bg_cat,
    ) = build_schedule_pattern(seed=i)
    score, raw_score, metrics = evaluate_schedule_with_raw(
        pattern_df,
        counts,
        bg_counts,
        ht_counts,
        wd_counts,
        we_counts,
        bk_counts,
        ly_counts,
    )

    score_rows.append({"seed": i, "score": score, "raw_score": raw_score, **metrics})

    # gapé•åãŒ0å€‹ã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿æ¡ç”¨ï¼ˆå®Œå…¨ãªgapåˆ¶ç´„éµå®ˆï¼‰
    gap_violations = metrics.get("gap_violations", 0)
    if gap_violations == 0:
        candidates.append({
            "seed": i,
            "score": score,
            "raw_score": raw_score,
            "metrics": metrics,
            "pattern_df": pattern_df,
        })

# gapé•å0å€‹ã®å€™è£œã‚’ã‚¹ã‚³ã‚¢é †ã«ã‚½ãƒ¼ãƒˆ
candidates = sorted(candidates, key=lambda e: e["raw_score"], reverse=True)[:TOP_KEEP]

print(f"\nâœ… {NUM_PATTERNS}ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ç”Ÿæˆå®Œäº†")
print(f"   gapé•å0å€‹ã®å€™è£œ: {len(candidates)}å€‹")
if len(candidates) == 0:
    print("   âš ï¸ è­¦å‘Š: gapé•å0å€‹ã®å€™è£œãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚åˆ¶ç´„ã‚’ç·©å’Œã—ã¾ã™...")
    # gapé•åã®åˆ¶ç´„ã‚’ç·©å’Œã—ã¦å†é¸æŠ
    candidates = []
    for row in score_rows:
        candidates.append({
            "seed": row["seed"],
            "score": row["score"],
            "raw_score": row["raw_score"],
            "metrics": {k: v for k, v in row.items() if k not in ["seed", "score", "raw_score"]},
            "pattern_df": None,  # å†ç”ŸæˆãŒå¿…è¦
        })
    candidates = sorted(candidates, key=lambda e: e["raw_score"], reverse=True)[:TOP_KEEP]
    # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å†ç”Ÿæˆ
    for cand in candidates:
        if cand["pattern_df"] is None:
            pattern_df, *_ = build_schedule_pattern(seed=cand["seed"])
            cand["pattern_df"] = pattern_df

print(f"   TOP{min(TOP_KEEP, len(candidates))}å€™è£œã‚’å±€æ‰€æ¢ç´¢ã§æœ€é©åŒ–ä¸­...")

# ãƒ­ãƒ¼ã‚«ãƒ«æ¢ç´¢ã§å€™è£œã‚’æ”¹å–„
refined = []
for idx, cand in enumerate(candidates[:REFINE_TOP], 1):
    print(f"   å€™è£œ{idx}/{REFINE_TOP}ã‚’æœ€é©åŒ–ä¸­...")
    if LOCAL_SEARCH_ENABLED:
        improved_df, sc2, raw2, met2 = local_search_swap(
            cand["pattern_df"],
            max_iters=LOCAL_MAX_ITERS,
            patience=LOCAL_PATIENCE,
            refresh_every=LOCAL_REFRESH_EVERY,
            seed=1000 + cand["seed"],
        )
    else:
        improved_df = cand["pattern_df"]
        sc2 = cand["score"]
        raw2 = cand["raw_score"]
        met2 = cand["metrics"]

    # ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã®è‡ªå‹•ä¿®æ­£
    print(f"   å€™è£œ{idx}/{REFINE_TOP}ã®ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åãƒã‚§ãƒƒã‚¯ä¸­...")
    fixed_df, fix_success, fix_count, fail_count = fix_hard_constraint_violations(
        improved_df, max_attempts=50, verbose=True
    )

    # TARGET_CAPé•åã®è‡ªå‹•ä¿®æ­£
    print(f"   å€™è£œ{idx}/{REFINE_TOP}ã®TARGET_CAPãƒã‚§ãƒƒã‚¯ä¸­...")
    cap_fixed_df, cap_success, cap_fix_count = fix_target_cap_violations(
        fixed_df, max_attempts=100, verbose=True
    )

    # å¯å¦ã‚³ãƒ¼ãƒ‰1.2ã®åŒ»å¸«ãŒå¤§å­¦ç³»0å›ã®é•åã‚’ä¿®æ­£
    print(f"   å€™è£œ{idx}/{REFINE_TOP}ã®å¯å¦ã‚³ãƒ¼ãƒ‰1.2ãƒã‚§ãƒƒã‚¯ä¸­...")
    code_1_2_fixed_df, code_1_2_success, code_1_2_fix_count = fix_code_1_2_violations(
        cap_fixed_df, max_attempts=100, verbose=True
    )

    # å¤§å­¦ç³»ã¨å¤–ç—…é™¢ã®å·®ãŒ3ä»¥ä¸Šã®é•åã‚’ä¿®æ­£
    print(f"   å€™è£œ{idx}/{REFINE_TOP}ã®å¤§å­¦ç³»/å¤–ç—…é™¢ãƒãƒ©ãƒ³ã‚¹ãƒã‚§ãƒƒã‚¯ä¸­...")
    bg_ht_fixed_df, bg_ht_success, bg_ht_fix_count = fix_bg_ht_imbalance_violations(
        code_1_2_fixed_df, max_attempts=100, verbose=True
    )

    # gapé•åï¼ˆ4æ—¥æœªæº€ã®é–“éš”ï¼‰ã‚’ä¿®æ­£
    print(f"   å€™è£œ{idx}/{REFINE_TOP}ã®gapé•åãƒã‚§ãƒƒã‚¯ä¸­...")
    gap_fixed_df, gap_success, gap_fix_count = fix_gap_violations(
        bg_ht_fixed_df, max_attempts=200, verbose=True
    )

    # å¤–ç—…é™¢é‡è¤‡ã‚’ä¿®æ­£ï¼ˆå„ªå…ˆåº¦3ä½ï¼‰
    print(f"   å€™è£œ{idx}/{REFINE_TOP}ã®å¤–ç—…é™¢é‡è¤‡ãƒã‚§ãƒƒã‚¯ä¸­...")
    ext_dup_fixed_df, ext_dup_success, ext_dup_fix_count = fix_external_hospital_dup_violations(
        gap_fixed_df, max_attempts=150, verbose=True
    )

    # å¤§å­¦3å›ä»¥ä¸Šé•åã‚’ä¿®æ­£
    print(f"   å€™è£œ{idx}/{REFINE_TOP}ã®å¤§å­¦3å›ä»¥ä¸Šãƒã‚§ãƒƒã‚¯ä¸­...")
    univ_over_2_fixed_df, univ_over_2_success, univ_over_2_fix_count = fix_university_over_2_violations(
        ext_dup_fixed_df, max_attempts=150, verbose=True
    )

    # å¤§å­¦å¹³æ—¥åã‚Šé•åã‚’ä¿®æ­£
    print(f"   å€™è£œ{idx}/{REFINE_TOP}ã®å¤§å­¦å¹³æ—¥åã‚Šãƒã‚§ãƒƒã‚¯ä¸­...")
    univ_weekday_fixed_df, univ_weekday_success, univ_weekday_fix_count = fix_university_weekday_balance_violations(
        univ_over_2_fixed_df, max_attempts=150, verbose=True
    )

    # 9. å…¬å¹³æ€§é•åã®ä¿®æ­£ï¼ˆæœ€å¤§ã¨æœ€å°ã®å·®ã‚’ç¸®ã‚ã‚‹ï¼‰
    fairness_fixed_df, fairness_success, fairness_fix_count = fix_fairness_imbalance(
        univ_weekday_fixed_df, max_attempts=200, verbose=True
    )

    # ä¿®æ­£å¾Œã«å†è©•ä¾¡
    if fix_count > 0 or cap_fix_count > 0 or code_1_2_fix_count > 0 or bg_ht_fix_count > 0 or gap_fix_count > 0 or ext_dup_fix_count > 0 or univ_over_2_fix_count > 0 or univ_weekday_fix_count > 0 or fairness_fix_count > 0:
        counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, *_ = recompute_stats(fairness_fixed_df)
        sc2, raw2, met2 = evaluate_schedule_with_raw(
            fairness_fixed_df, counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts
        )
        improved_df = fairness_fixed_df
    else:
        improved_df = fairness_fixed_df

    refined.append({
        "seed": cand["seed"],
        "score_before": cand["score"],
        "raw_before": cand["raw_score"],
        "score_after": sc2,
        "raw_after": raw2,
        "metrics_after": met2,
        "pattern_df": improved_df,
        "violations_fixed": fix_count,
        "violations_failed": fail_count,
        "cap_violations_fixed": cap_fix_count,
        "code_1_2_violations_fixed": code_1_2_fix_count,
        "bg_ht_imbalance_fixed": bg_ht_fix_count,
        "gap_violations_fixed": gap_fix_count,
        "external_dup_violations_fixed": ext_dup_fix_count,
        "univ_over_2_violations_fixed": univ_over_2_fix_count,
        "univ_weekday_violations_fixed": univ_weekday_fix_count,
        "fairness_violations_fixed": fairness_fix_count,
    })

# =========================
# ãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã®ãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿é¸æŠï¼ˆTARGET_CAPã€gapã€æœªå‰²å½“ï¼‰
# =========================
print("\n=== ãƒãƒ¼ãƒ‰åˆ¶ç´„ãƒã‚§ãƒƒã‚¯ ===")
valid_patterns = []
for e in refined:
    met = e["metrics_after"]
    cap_viol = met.get('cap_violations', 0)
    gap_viol = met.get('gap_violations', 0)
    unassigned = met.get('unassigned_slots', 0)

    if cap_viol > 0 or gap_viol > 0 or unassigned > 0:
        print(f"   âŒ seed={e['seed']}: TARGET_CAPé•å={cap_viol}, gapé•å={gap_viol}, æœªå‰²å½“={unassigned} â†’ é™¤å¤–")
    else:
        valid_patterns.append(e)

if not valid_patterns:
    print("   âš ï¸ è­¦å‘Š: ãƒãƒ¼ãƒ‰åˆ¶ç´„ã‚’æº€ãŸã™ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…¨ãƒ‘ã‚¿ãƒ¼ãƒ³ã‹ã‚‰é¸æŠã—ã¾ã™ã€‚")
    valid_patterns = refined

print(f"   âœ… {len(valid_patterns)}/{len(refined)} ãƒ‘ã‚¿ãƒ¼ãƒ³ãŒãƒãƒ¼ãƒ‰åˆ¶ç´„ã‚’æº€ãŸã—ã¦ã„ã¾ã™")

# =========================
# å¤šè»¸ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚°: ç•°ãªã‚‹è©•ä¾¡è»¸ã§æœ€é©ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸æŠ
# =========================
print("\n=== å¤šè»¸ã‚¹ã‚³ã‚¢ãƒªãƒ³ã‚° ===")

# è©•ä¾¡è»¸1: å…¬å¹³æ€§é‡è¦–ï¼ˆTARGET_CAPã€å…¬å¹³æ€§ãƒšãƒŠãƒ«ãƒ†ã‚£ã‚’é‡è¦–ï¼‰
fairness_patterns = sorted(
    valid_patterns,
    key=lambda e: (
        -e["metrics_after"].get('cap_violations', 0) * 1000,  # TARGET_CAPé•åã‚’æœ€å„ªå…ˆã§å›é¿
        -e["metrics_after"].get('max_minus_min_total_active', 0) * 100,  # å…¬å¹³æ€§
        -e["metrics_after"].get('bg_ht_imbalance_violations', 0) * 50,
        e["raw_after"]
    ),
    reverse=True
)

# è©•ä¾¡è»¸2: gapé•åå›é¿é‡è¦–ï¼ˆé€£ç¶šå½“ç›´ã®é–“éš”ã‚’é‡è¦–ï¼‰
gap_patterns = sorted(
    valid_patterns,
    key=lambda e: (
        -e["metrics_after"].get('gap_violations', 0) * 1000,
        -e["metrics_after"].get('external_hosp_dup_violations', 0) * 100,
        -e["metrics_after"].get('hospital_dup_violations', 0) * 50,
        e["raw_after"]
    ),
    reverse=True
)

# è©•ä¾¡è»¸3: ãƒãƒ©ãƒ³ã‚¹é‡è¦–ï¼ˆå¤§å­¦/å¤–ç—…é™¢ã€å¹³æ—¥/ä¼‘æ—¥ã®ãƒãƒ©ãƒ³ã‚¹ã‚’é‡è¦–ï¼‰
balance_patterns = sorted(
    valid_patterns,
    key=lambda e: (
        -e["metrics_after"].get('bg_ht_imbalance_violations', 0) * 1000,
        -e["metrics_after"].get('bg_weekday_weekend_imbalance', 0) * 100,
        -e["metrics_after"].get('bg_over_2_violations', 0) * 100,
        -e["metrics_after"].get('bg_weekday_over_violations', 0) * 100,
        e["raw_after"]
    ),
    reverse=True
)

# å„è»¸ã‹ã‚‰æœ€è‰¯ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸æŠ
top_patterns = []
selected_seeds = set()

# è»¸1: å…¬å¹³æ€§é‡è¦–
if fairness_patterns and fairness_patterns[0]["seed"] not in selected_seeds:
    fairness_patterns[0]["axis_label"] = "å…¬å¹³æ€§é‡è¦–"
    top_patterns.append(fairness_patterns[0])
    selected_seeds.add(fairness_patterns[0]["seed"])

# è»¸2: gapé•åå›é¿é‡è¦–
if gap_patterns and gap_patterns[0]["seed"] not in selected_seeds:
    gap_patterns[0]["axis_label"] = "é€£ç¶šå½“ç›´å›é¿é‡è¦–"
    top_patterns.append(gap_patterns[0])
    selected_seeds.add(gap_patterns[0]["seed"])

# è»¸3: ãƒãƒ©ãƒ³ã‚¹é‡è¦–
if balance_patterns and balance_patterns[0]["seed"] not in selected_seeds:
    balance_patterns[0]["axis_label"] = "ãƒãƒ©ãƒ³ã‚¹é‡è¦–"
    top_patterns.append(balance_patterns[0])
    selected_seeds.add(balance_patterns[0]["seed"])

# ã¾ã 3ãƒ‘ã‚¿ãƒ¼ãƒ³æœªæº€ã®å ´åˆã€ç·åˆã‚¹ã‚³ã‚¢ã‹ã‚‰è£œå¡«
if len(top_patterns) < 3:
    overall_sorted = sorted(valid_patterns, key=lambda e: e["raw_after"], reverse=True)
    for pattern in overall_sorted:
        if pattern["seed"] not in selected_seeds:
            pattern["axis_label"] = "ç·åˆã‚¹ã‚³ã‚¢"
            top_patterns.append(pattern)
            selected_seeds.add(pattern["seed"])
            if len(top_patterns) >= 3:
                break

# ã‚½ãƒ¼ãƒˆæ¸ˆã¿ãƒªã‚¹ãƒˆã‚‚ä½œæˆï¼ˆå¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ï¼‰
refined_sorted = sorted(valid_patterns, key=lambda e: e["raw_after"], reverse=True)
TOP_OUTPUT_PATTERNS = len(top_patterns)

scores_df = pd.DataFrame(score_rows).sort_values(["raw_score", "seed"], ascending=[False, True]).reset_index(drop=True)

refined_df = pd.DataFrame([
    {
        "seed": e["seed"],
        "score_before": e["score_before"],
        "raw_before": e["raw_before"],
        "score_after": e["score_after"],
        "raw_after": e["raw_after"],
        **{f"after_{k}": v for k, v in e["metrics_after"].items() if k not in ("raw_score", "penalty_total")},
    }
    for e in refined_sorted
]).sort_values(["raw_after", "seed"], ascending=[False, True]).reset_index(drop=True)

print("\nâœ… å±€æ‰€æ¢ç´¢å®Œäº†")
print("\n=== TOPãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚¹ã‚³ã‚¢ï¼ˆå¤šè»¸è©•ä¾¡ï¼‰ ===")
for rank, pattern in enumerate(top_patterns, 1):
    axis_label = pattern.get('axis_label', 'ç·åˆã‚¹ã‚³ã‚¢')
    print(
        f"   {rank}ä½ [{axis_label}]: raw_score={pattern['raw_after']:.1f}, "
        + f"gapé•å={pattern['metrics_after']['gap_violations']}, "
        + f"æœªå‰²å½“={pattern['metrics_after']['unassigned_slots']}, "
        + f"capé•å={pattern['metrics_after'].get('cap_violations', 0)}, "
        + f"1.2é•å={pattern['metrics_after'].get('code_1_2_violations', 0)}, "
        + f"BG/HTå·®3ä»¥ä¸Š={pattern['metrics_after'].get('bg_ht_imbalance_violations', 0)}, "
        + f"å…¬å¹³æ€§(max-min)={pattern['metrics_after'].get('max_minus_min_total_active', 0)}, "
        + f"åˆ¶ç´„ä¿®æ­£={pattern.get('violations_fixed', 0)}ä»¶"
    )

# =========================
# å‡ºåŠ›ï¼ˆpattern + summary + diagnosticsï¼‰
# =========================
base_name = uploaded_filename.rsplit(".", 1)[0]
output_filename = f"{base_name}_auto_schedules_v2.8.xlsx"
output_path = output_filename

print(f"\nğŸ“ çµæœã‚’Excelãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ä¸­...")

def write_diagnostics_sheet(writer, sheet_name, diagnostics):
    startrow = 0
    for title, df in diagnostics:
        df.to_excel(writer, sheet_name=sheet_name, startrow=startrow + 1, index=False)
        ws = writer.sheets[sheet_name]
        ws.cell(row=startrow + 1, column=1, value=title)
        startrow += len(df.index) + 3


with pd.ExcelWriter(output_path, engine="openpyxl") as writer:
    # å…ƒã‚·ãƒ¼ãƒˆ
    shift_df.to_excel(writer, sheet_name="sheet1", index=False)
    availability_raw.to_excel(writer, sheet_name="sheet2", index=False)
    schedule_raw.to_excel(writer, sheet_name="sheet3", index=False)
    sheet4_raw_out.to_excel(writer, sheet_name="sheet4", index=False)

    # TOPãƒ‘ã‚¿ãƒ¼ãƒ³å‡ºåŠ›
    for rank, entry in enumerate(top_patterns, start=1):
        axis_label = entry.get('axis_label', 'ç·åˆã‚¹ã‚³ã‚¢')
        sheet_label = f"pattern_{rank:02d}"

        # ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚·ãƒ¼ãƒˆã®ã‚³ãƒ¡ãƒ³ãƒˆè¡Œã«è»¸ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ 
        pattern_df_with_label = entry["pattern_df"].copy()
        entry["pattern_df"].to_excel(writer, sheet_name=sheet_label, index=False)

        # ã‚·ãƒ¼ãƒˆåã«è»¸ãƒ©ãƒ™ãƒ«ã‚’è¿½åŠ ï¼ˆExcelã®åˆ¶é™ã«ã‚ˆã‚Šç°¡ç•¥åŒ–ï¼‰
        ws = writer.sheets[sheet_label]
        axis_short = {"å…¬å¹³æ€§é‡è¦–": "å…¬å¹³æ€§", "é€£ç¶šå½“ç›´å›é¿é‡è¦–": "gapå›é¿", "ãƒãƒ©ãƒ³ã‚¹é‡è¦–": "ãƒãƒ©ãƒ³ã‚¹", "ç·åˆã‚¹ã‚³ã‚¢": "ç·åˆ"}.get(axis_label, axis_label)
        ws.cell(row=1, column=len(entry["pattern_df"].columns) + 2, value=f"ã€{axis_short}ã€‘")

        # summaryï¼ˆä»Šæœˆ/ç´¯è¨ˆï¼‰
        counts, bg_counts, ht_counts, wd_counts, we_counts, bk_counts, ly_counts, bg_cat, *_ = recompute_stats(entry["pattern_df"])
        df_month, df_total = build_summaries(entry["pattern_df"], counts, bg_counts, ht_counts, wd_counts, we_counts, bg_cat)
        df_month.to_excel(writer, sheet_name=f"{sheet_label}_ä»Šæœˆ", index=False)
        df_total.to_excel(writer, sheet_name=f"{sheet_label}_ç´¯è¨ˆ", index=False)

        # diagnostics
        df_doctors, df_gap, df_same, df_hdup, df_unass, df_metrics, df_hard_violations = build_diagnostics(entry["pattern_df"])
        write_diagnostics_sheet(
            writer,
            sheet_name=f"{sheet_label}_diag",
            diagnostics=[
                ("ğŸš¨ ãƒãƒ¼ãƒ‰åˆ¶ç´„é•å", df_hard_violations),
                ("åŒ»å¸«ã”ã¨ã®åã‚Š", df_doctors),
                ("gapé•å", df_gap),
                ("åŒæ—¥é‡è¤‡", df_same),
                ("åŒä¸€ç—…é™¢é‡è¤‡", df_hdup),
                ("æœªå‰²å½“æ ", df_unass),
                ("ãƒ¡ãƒˆãƒªã‚¯ã‚¹", df_metrics),
            ],
        )

print("\n" + "="*60)
print("   ğŸ‰ å®Œäº†ï¼")
print("="*60)
print(f"\nğŸ“¥ å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«: {output_path}")
print("\nã€ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã€‘")
print("  - sheet1ã€œ4: å…ƒãƒ‡ãƒ¼ã‚¿")
print("  - pattern_01ã€œ03: å¤šè»¸è©•ä¾¡ã«ã‚ˆã‚‹TOP3ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«å€™è£œ")
print("    * å…¬å¹³æ€§é‡è¦–: åŒ»å¸«é–“ã®å‰²å½“å›æ•°ã®å…¬å¹³æ€§ã‚’æœ€å„ªå…ˆ")
print("    * gapå›é¿é‡è¦–: é€£ç¶šå½“ç›´ã®é–“éš”ã‚’æœ€å„ªå…ˆ")
print("    * ãƒãƒ©ãƒ³ã‚¹é‡è¦–: å¤§å­¦/å¤–ç—…é™¢ã€å¹³æ—¥/ä¼‘æ—¥ã®ãƒãƒ©ãƒ³ã‚¹ã‚’æœ€å„ªå…ˆ")
print("  - pattern_XX_ä»Šæœˆ/ç´¯è¨ˆ: å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã‚µãƒãƒªãƒ¼ã‚·ãƒ¼ãƒˆ")
print("  - pattern_XX_diag: å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¨ºæ–­ã‚·ãƒ¼ãƒˆï¼ˆãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã€gapé•åã€é‡è¤‡ç­‰ï¼‰")
print("\nã€æ¨å¥¨ã€‘")
print("  âœ… å¤šè»¸è©•ä¾¡ã«ã‚ˆã‚Šç•°ãªã‚‹ç‰¹æ€§ã‚’æŒã¤ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æä¾›")
print("  âœ… TARGET_CAPã€gapã€æœªå‰²å½“ã®é•åãŒãªã„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ã¿é¸æŠ")
print("  ğŸ” å„pattern_XX_diagã®ã€Œãƒãƒ¼ãƒ‰åˆ¶ç´„é•åã€ã‚·ãƒ¼ãƒˆã§ä¿®æ­£çµæœã‚’ç¢ºèª")
print("  1. 3ã¤ã®è©•ä¾¡è»¸ï¼ˆå…¬å¹³æ€§/gapå›é¿/ãƒãƒ©ãƒ³ã‚¹ï¼‰ã‹ã‚‰æœ€é©ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é¸æŠ")
print("  2. é¸æŠã—ãŸãƒ‘ã‚¿ãƒ¼ãƒ³ã®è¨ºæ–­ã‚·ãƒ¼ãƒˆã§é•åãƒ»é‡è¤‡ã‚’ç¢ºèª")
print("  3. ã‚µãƒãƒªãƒ¼ã‚·ãƒ¼ãƒˆã§åŒ»å¸«ã”ã¨ã®åã‚Šã‚’ç¢ºèª")
print("\nã€ä¸»ãªè‡ªå‹•ä¿®æ­£å¯¾è±¡ã€‘")
print("  âœ… å¯å¦ã‚³ãƒ¼ãƒ‰0é•åï¼ˆçµ¶å¯¾ä¸å¯ã®æ—¥ã«å‰²å½“ï¼‰")
print("  âœ… ã‚«ãƒ†è¡¨+å¤–ç—…é™¢é•åï¼ˆã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ãŒã‚ã‚‹æ—¥ã«Lã€œYåˆ—ã«å‰²å½“ï¼‰â† äº”ååµåŒ»å¸«ã®å•é¡Œã‚’ä¿®æ­£")
print("  âœ… å¯å¦ã‚³ãƒ¼ãƒ‰2é•åï¼ˆBã€œQåˆ—ä»¥å¤–ã«å‰²å½“ï¼‰")
print("  âœ… å¯å¦ã‚³ãƒ¼ãƒ‰3é•åï¼ˆLã€œYåˆ—ä»¥å¤–ã«å‰²å½“ï¼‰")
print("  âœ… B-Kåˆ—ã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰æ¬ å¦‚ï¼ˆã‚«ãƒ†è¡¨ã‚³ãƒ¼ãƒ‰ä¿æœ‰åŒ»å¸«ãŒã‚³ãƒ¼ãƒ‰ãªã—æ—¥ã«Bã€œKåˆ—ã«å‰²å½“ï¼‰")
print("  âœ… æ°´æ›œæ—¥Lã€œYåˆ—ç¦æ­¢é•å")
print("="*60)

if COLAB_AVAILABLE:
    files.download(output_path)
